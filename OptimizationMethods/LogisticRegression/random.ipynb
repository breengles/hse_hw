{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimization import optimize\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from oracle import Oracle, make_oracle\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def optimize_sklearn(oracle):\n",
    "    model = LogisticRegression(penalty=\"none\", tol=1e-8, max_iter=10000, n_jobs=-1, fit_intercept=False)\n",
    "    model.fit(oracle.X, oracle.Y.ravel())\n",
    "\n",
    "    entropy_true = oracle.value(model.coef_.reshape(-1, 1))\n",
    "    return entropy_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 4000\n",
    "num_features = 50\n",
    "\n",
    "w = np.random.uniform(-1, 1, size=(num_features,1))\n",
    "oracle = make_oracle(size=num_points, w=w)\n",
    "true_ans = optimize_sklearn(oracle)\n",
    "\n",
    "w0 = np.zeros((oracle.m, 1))\n",
    "\n",
    "to_df = []\n",
    "optimization_methods = [\"gradient_descent\", \"newton\", \"conjugate_gradient\"]\n",
    "linesearch_methods = [\"golden_section\", \"brent\", \"dbrent\", \"armijo\", \"wolfe\"]\n",
    "for opt in optimization_methods:\n",
    "    if opt == \"gradient_descent\":\n",
    "        to_iter = linesearch_methods + [\"nesterov\"]\n",
    "    else:\n",
    "        to_iter = linesearch_methods\n",
    "        \n",
    "    for ls in to_iter:\n",
    "        if ls == \"armijo\":\n",
    "            c1 = 0.25\n",
    "            c2 = None\n",
    "        elif ls == \"wolfe\":\n",
    "            c1 = 1e-4\n",
    "            c2 = 0.9\n",
    "        elif ls == \"nesterov\":\n",
    "            c1 = c2 = 2.0\n",
    "        else:\n",
    "            c1 = 0.25\n",
    "            c2 = None\n",
    "\n",
    "        _, _, log = optimize(w0, oracle, opt, ls, output_log=True, c1=c1, c2=c2, tol=1e-30)\n",
    "        to_df.append(log.best)\n",
    "\n",
    "df = pd.concat(to_df, ignore_index=True)\n",
    "table = [df.columns.values.tolist()] + df.values.tolist()\n",
    "print(f\"- {num_points} x {num_features} {true_ans}-\")\n",
    "print(tabulate(table, headers=\"firstrow\", tablefmt=\"github\", floatfmt=[\"\", \"\", \".0e\", \".2e\", \".2e\", \"\", \".2e\", \"\", \".4f\", \".1e\"]))"
   ]
  },
  {
   "source": [
    "Рандомный датасет на $4000$ точек и $50$ фичей. Значение энтропии из **sklearn** = 3.98039543e-12"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| OptMethod          | LineSearch     |   tol |       c1 |       c2 |                entropy |   num_iter |   oracle_calls |     time |      rk |\n",
    "|--------------------|----------------|-------|----------|----------|------------------------|------------|----------------|----------|---------|\n",
    "| gradient_descent   | golden_section | 1e-30 | 2.50e-01 |          |  0.0009587512255371034 |      10000 |         465006 |  72.4897 | 2.3e-09 |\n",
    "| gradient_descent   | brent          | 1e-30 | 2.50e-01 |          |  0.0009580380514109347 |      10000 |         232481 |  40.0178 | 2.3e-09 |\n",
    "| gradient_descent   | dbrent         | 1e-30 | 2.50e-01 |          |  0.0009587523801053996 |      10000 |         319433 | 191.1798 | 2.3e-09 |\n",
    "| gradient_descent   | armijo         | 1e-30 | 2.50e-01 |          |  0.0002495345717689766 |      10000 |          87420 |  22.2659 | 2.1e-10 |\n",
    "| gradient_descent   | wolfe          | 1e-30 | 1.00e-04 | 9.00e-01 |  0.0006462985712436903 |      10000 |         214522 |  70.0297 | 6.1e-10 |\n",
    "| gradient_descent   | nesterov       | 1e-30 | 2.00e+00 | 2.00e+00 |  0.000677562148132333  |      10000 |          39987 |  10.8523 | 7.7e-10 |\n",
    "| newton             | golden_section | 1e-30 | 2.50e-01 |          | -9.951572343953082e-13 |         39 |            115 |   0.1067 | 7.0e-31 |\n",
    "| newton             | brent          | 1e-30 | 2.50e-01 |          | -9.951572343953082e-13 |         39 |            115 |   0.1102 | 7.0e-31 |\n",
    "| newton             | dbrent         | 1e-30 | 2.50e-01 |          | -9.951572343953082e-13 |         39 |            115 |   0.1292 | 7.0e-31 |\n",
    "| newton             | armijo         | 1e-30 | 2.50e-01 |          | -9.951572343953082e-13 |         39 |            115 |   0.1127 | 7.0e-31 |\n",
    "| newton             | wolfe          | 1e-30 | 1.00e-04 | 9.00e-01 | -9.951572343953082e-13 |         39 |            115 |   0.1072 | 7.0e-31 |\n",
    "| conjugate_gradient | golden_section | 1e-30 | 2.50e-01 |          |  3.073073741203821e-10 |         28 |             82 |   0.2319 | 3.8e-32 |\n",
    "| conjugate_gradient | brent          | 1e-30 | 2.50e-01 |          |  3.073073741203821e-10 |         28 |             82 |   0.2349 | 3.8e-32 |\n",
    "| conjugate_gradient | dbrent         | 1e-30 | 2.50e-01 |          |  3.073073741203821e-10 |         28 |             82 |   0.2399 | 3.8e-32 |\n",
    "| conjugate_gradient | armijo         | 1e-30 | 2.50e-01 |          |  3.073073741203821e-10 |         28 |             82 |   0.2573 | 3.8e-32 |\n",
    "| conjugate_gradient | wolfe          | 1e-30 | 1.00e-04 | 9.00e-01 |  3.073073741203821e-10 |         28 |             82 |   0.2748 | 3.8e-32 |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "В среднем, сложно что-то сказать: градиентный спуск не сошелся за $10000$ итераций, сопряженные градиенты сколько-то приблизилист к правильному ответу, а Ньютон сошел с ума и упал ниже подвала."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}