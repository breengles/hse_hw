{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from itertools import product\n",
    "import joblib\n",
    "from os import makedirs\n",
    "import uuid\n",
    "from train import *\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations, product, permutations\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env_name=\"AntBulletEnv-v0\", \n",
    "        transitions=1000000, \n",
    "        eps=0.2, \n",
    "        gamma=0.99, \n",
    "        tau=0.002, \n",
    "        actor_lr=2e-4, \n",
    "        critic_lr=5e-4, \n",
    "        sigma=2, \n",
    "        c=1, \n",
    "        updates_number=1, \n",
    "        policy_delay=1, \n",
    "        batch_size=128,\n",
    "        buffer_size=200000,\n",
    "        start_training=None,\n",
    "        evaluate_every=None,\n",
    "        seed=42):\n",
    "    start_training = start_training if start_training else buffer_size // 10\n",
    "    evaluate_every = evaluate_every if evaluate_every else transitions // 100\n",
    "    log = {\n",
    "        \"cfg\": {\n",
    "            \"transitions\": transitions,\n",
    "            \"eps\": eps,\n",
    "            \"gamma\": gamma,\n",
    "            \"tau\": tau,\n",
    "            \"actor_lr\": actor_lr,\n",
    "            \"critic_lr\": critic_lr,\n",
    "            \"sigma\": sigma,\n",
    "            \"c\": c,\n",
    "            \"updates_number\": updates_number,\n",
    "            \"policy_delay\": policy_delay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"start_training\": start_training\n",
    "        },\n",
    "        \"step\": [],\n",
    "        \"rmean\": [],\n",
    "        \"rstd\": []\n",
    "    }\n",
    "\n",
    "    makedirs(\"experiments\", exist_ok=True)\n",
    "    saved_agent_dir = \"experiments/\" + str(uuid.uuid4())\n",
    "    makedirs(saved_agent_dir)\n",
    "    with open(f\"{saved_agent_dir}/params.json\", \"w\") as param:\n",
    "        json.dump(log[\"cfg\"], param, indent=4)\n",
    "    log_file = open(f\"{saved_agent_dir}/log.csv\", \"a\")\n",
    "\n",
    "    env = make(env_name)\n",
    "    test_env = make(env_name)\n",
    "    \n",
    "    td3 = TD3(state_dim=env.observation_space.shape[0], \n",
    "              action_dim=env.action_space.shape[0], \n",
    "              actor_lr=actor_lr, critic_lr=critic_lr,\n",
    "              buffer_size=buffer_size)\n",
    "\n",
    "    state = env.reset()\n",
    "    episodes_sampled = 0\n",
    "    steps_sampled = 0\n",
    "    \n",
    "    set_seed(env, seed=seed)\n",
    "\n",
    "    t = tqdm(range(transitions))\n",
    "\n",
    "    for i in t:\n",
    "        if i > start_training:\n",
    "            action = td3.act(state)\n",
    "            action = np.clip(action + eps * np.random.randn(*action.shape), -1, +1)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            td3.update((state, action, next_state, reward, done), \n",
    "                        sigma=sigma, c=c, updates_number=updates_number, policy_delay=policy_delay,\n",
    "                        batch_size=batch_size, gamma=gamma, tau= tau)\n",
    "        else:\n",
    "            action = np.random.uniform(-1, 1, size=env.action_space.shape)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td3.replay_buffer.append((state, action, next_state, reward, done))\n",
    "\n",
    "        state = env.reset() if done else next_state\n",
    "        \n",
    "        if (i + 1) % evaluate_every == 0:\n",
    "            rewards = evaluate_policy(test_env, td3, 5, seed=seed)\n",
    "            rmean = np.mean(rewards)\n",
    "            rstd = np.std(rewards)\n",
    "\n",
    "            log[\"step\"].append(i + 1)\n",
    "            log[\"rmean\"].append(rmean)\n",
    "            log[\"rstd\"].append(rstd)\n",
    "\n",
    "            log_file.write(f\"{i + 1},{rmean},{rstd}\\n\")\n",
    "            log_file.flush()\n",
    "            \n",
    "            if rmean > 2500:\n",
    "                td3.save(name=f\"{saved_agent_dir}/{i + 1}_{int(rmean)}_{int(rstd)}.pkl\")\n",
    "\n",
    "            t.set_description(f\"{rmean:0.2f} | {rstd:0.2f}\")\n",
    "\n",
    "    return log\n",
    "\n",
    "drun = delayed(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(log):\n",
    "    rmean = np.array(log[\"rmean\"])\n",
    "    rstd = np.array(log[\"rstd\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    cfg = log[\"cfg\"]\n",
    "    ax.set_title(f\"{cfg}\")\n",
    "    ax.set_xlabel(\"â„– of transitions\")\n",
    "    ax.set_ylabel(\"Mean reward\")\n",
    "\n",
    "    plt.hlines(2000, np.min(log[\"step\"]), np.max(log[\"step\"]),\n",
    "               colors=\"r\", label=\"Solved\")\n",
    "\n",
    "    plt.plot(log[\"step\"],\n",
    "             rmean,\n",
    "             label=\"TD3\")                \n",
    "\n",
    "\n",
    "    plt.fill_between(log[\"step\"],\n",
    "                     rmean - rstd,\n",
    "                     rmean + rstd, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"transitions\": 10_000_000,\n",
    "    \"eps\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.002,\n",
    "    \"actor_lr\": 2e-4,\n",
    "    \"critic_lr\": 5e-4,\n",
    "    \"sigma\": 2,\n",
    "    \"c\": 0.5,\n",
    "    \"updates_number\": 1,\n",
    "    \"policy_delay\": 1,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 200_000,\n",
    "    \"start_training\": -1,\n",
    "    \"evaluate_every\": 10_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(run(**base_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = []\n",
    "\n",
    "params = product(\n",
    "    (1_000_000,),    # transitions\n",
    "    (0.2, 0.5),      # eps\n",
    "    (0.999, 0.99),   # gamma\n",
    "    (1e-3, 1e-2),    # tau\n",
    "    (1e-4, 1e-3),    # actor lr\n",
    "    (2e-4, 2e-3),    # critic lr\n",
    "    (2, 1, 0.5),     # sigma\n",
    "    (2, 1, 0.5),     # c\n",
    "    (1, 2, 4, 8),    # number of updates\n",
    "    (1, 2, 4, 8),    # policy delay\n",
    "    (64, 128, 256),  # batch size\n",
    ")\n",
    "\n",
    "for tramsitions, eps, gamma, tau, actor_lr, critic_lr, sigma, c, updates_number, policy_delay, batch_size in params:\n",
    "    configs.append({\n",
    "        \"transitions\": tramsitions,\n",
    "        \"eps\": eps,\n",
    "        \"gamma\": gamma,\n",
    "        \"tau\": tau,\n",
    "        \"actor_lr\": actor_lr,\n",
    "        \"critic_lr\": critic_lr,\n",
    "        \"sigma\": sigma,\n",
    "        \"c\": c,\n",
    "        \"updates_number\": updates_number,\n",
    "        \"policy_delay\": policy_delay,\n",
    "        \"batch_size\": batch_size\n",
    "    })\n",
    "\n",
    "print(len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logs = Parallel(n_jobs=5)(drun(**cfg) for cfg in configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log in logs:\n",
    "    plot(log)"
   ]
  }
 ]
}