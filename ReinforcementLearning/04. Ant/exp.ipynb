{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from itertools import product\n",
    "import joblib\n",
    "from os import makedirs\n",
    "import uuid\n",
    "from train import *\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import combinations, product, permutations\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from utils.logger import Logger\n",
    "\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env_name=\"AntBulletEnv-v0\", \n",
    "        transitions=1000000, \n",
    "        eps=0.2, \n",
    "        gamma=0.99, \n",
    "        tau=0.002, \n",
    "        actor_lr=2e-4, \n",
    "        critic_lr=5e-4, \n",
    "        sigma=2, \n",
    "        c=1, \n",
    "        updates_number=1, \n",
    "        policy_delay=1, \n",
    "        batch_size=128,\n",
    "        buffer_size=200000,\n",
    "        start_training=None,\n",
    "        evaluate_every=None,\n",
    "        seed=42):\n",
    "    start_training = start_training if start_training else buffer_size // 10\n",
    "    evaluate_every = evaluate_every if evaluate_every else transitions // 100\n",
    "    \n",
    "    logger = Logger(locals())\n",
    "\n",
    "    makedirs(\"experiments\", exist_ok=True)\n",
    "    saved_agent_dir = \"experiments/\" + str(uuid.uuid4()) + \"/\"\n",
    "    makedirs(saved_agent_dir)\n",
    "\n",
    "    logger.save_params(saved_agent_dir + \"params.json\")\n",
    "\n",
    "    env = make(env_name)\n",
    "    test_env = make(env_name)\n",
    "    \n",
    "    td3 = TD3(state_dim=env.observation_space.shape[0], \n",
    "              action_dim=env.action_space.shape[0], \n",
    "              actor_lr=actor_lr, critic_lr=critic_lr,\n",
    "              buffer_size=buffer_size)\n",
    "\n",
    "    state = env.reset()\n",
    "    episodes_sampled = 0\n",
    "    steps_sampled = 0\n",
    "    \n",
    "    set_seed(env, seed=seed)\n",
    "\n",
    "    t = tqdm(range(transitions))\n",
    "\n",
    "    for i in t:\n",
    "        if i > start_training:\n",
    "            action = td3.act(state)\n",
    "            action = np.clip(action + eps * np.random.randn(*action.shape), -1, +1)\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            td3.update((state, action, next_state, reward, done), \n",
    "                        sigma=sigma, c=c, updates_number=updates_number, policy_delay=policy_delay,\n",
    "                        batch_size=batch_size, gamma=gamma, tau= tau)\n",
    "        else:\n",
    "            action = np.random.uniform(-1, 1, size=env.action_space.shape)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td3.replay_buffer.append((state, action, next_state, reward, done))\n",
    "\n",
    "        state = env.reset() if done else next_state\n",
    "        \n",
    "        if (i + 1) % evaluate_every == 0:\n",
    "            rewards = evaluate_policy(test_env, td3, 5, seed=seed)\n",
    "            rmean = np.mean(rewards)\n",
    "            rstd = np.std(rewards)\n",
    "\n",
    "            logger.log(\"step\", i + 1)\n",
    "            logger.log(\"rmean\", rmean)\n",
    "            logger.log(\"rstd\", rstd)\n",
    "            logger.save(saved_agent_dir + \"log.csv\")\n",
    "            \n",
    "            if rmean > 2500:\n",
    "                td3.save(name=f\"{saved_agent_dir}/{i + 1}_{int(rmean)}_{int(rstd)}.pkl\")\n",
    "\n",
    "            t.set_description(f\"{rmean:0.2f} | {rstd:0.2f}\")\n",
    "\n",
    "    return logger\n",
    "\n",
    "drun = delayed(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"transitions\": 1_000_000,\n",
    "    \"eps\": 0.1,\n",
    "    \"gamma\": 0.99,\n",
    "    \"tau\": 0.002,\n",
    "    \"actor_lr\": 2e-4,\n",
    "    \"critic_lr\": 5e-4,\n",
    "    \"sigma\": np.sqrt(2),\n",
    "    \"c\": 0.5,\n",
    "    \"updates_number\": 1,\n",
    "    \"policy_delay\": 1,\n",
    "    \"batch_size\": 2048,\n",
    "    \"buffer_size\": 200_000,\n",
    "    \"start_training\": -1,\n",
    "    \"evaluate_every\": None,\n",
    "}\n",
    "\n",
    "run(**base_config).plot(\"step\", \"rmean\", \"rstd\", label=\"TD3\", y_solved=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blacklist = [\n",
    "#     {\n",
    "#         \"transitions\": 1500000,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 4,\n",
    "#         \"batch_size\": 64,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": 1500000,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 1,\n",
    "#         \"batch_size\": 64,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": 1500000,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 1,\n",
    "#         \"batch_size\": 128,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": 1500000,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 2,\n",
    "#         \"batch_size\": 64,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": 1500000,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 8,\n",
    "#         \"batch_size\": 64,\n",
    "#         \"start_training\": 20000\n",
    "#     }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = []\n",
    "\n",
    "# params = product(\n",
    "#     (1_500_000,),    # transitions\n",
    "#     (0.2, 0.5, 1),      # eps\n",
    "#     (0.999, 0.99),   # gamma\n",
    "#     (1e-3, 1e-2),    # tau\n",
    "#     (1e-4, 1e-3),    # actor lr\n",
    "#     (2e-4, 2e-3),    # critic lr\n",
    "#     (2, 1, 0.5),     # sigma\n",
    "#     (2, 1, 0.5),     # c\n",
    "#     (1, 2, 4, 8),    # number of updates\n",
    "#     (1, 2, 4, 8),    # policy delay\n",
    "#     (64, 128, 256),  # batch size\n",
    "# )\n",
    "\n",
    "# for tramsitions, eps, gamma, tau, actor_lr, critic_lr, sigma, c, updates_number, policy_delay, batch_size in params:\n",
    "#     cfg = {\n",
    "#         \"transitions\": tramsitions,\n",
    "#         \"eps\": eps,\n",
    "#         \"gamma\": gamma,\n",
    "#         \"tau\": tau,\n",
    "#         \"actor_lr\": actor_lr,\n",
    "#         \"critic_lr\": critic_lr,\n",
    "#         \"sigma\": sigma,\n",
    "#         \"c\": c,\n",
    "#         \"updates_number\": updates_number,\n",
    "#         \"policy_delay\": policy_delay,\n",
    "#         \"batch_size\": batch_size\n",
    "#     }\n",
    "    \n",
    "#     if cfg not in blacklist:\n",
    "#         configs.append(cfg)\n",
    "\n",
    "# print(len(configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs = Parallel(n_jobs=5)(drun(**cfg) for cfg in tqdm(configs))\n",
    "# for log in logs:\n",
    "#     plot(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_num = 10_000_000\n",
    "# to_check = [\n",
    "#     {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 2,\n",
    "#         \"batch_size\": 128,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#         {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 1,\n",
    "#         \"batch_size\": 256,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 4,\n",
    "#         \"batch_size\": 64,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 4,\n",
    "#         \"batch_size\": 256,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 2,\n",
    "#         \"batch_size\": 256,\n",
    "#         \"start_training\": 20000\n",
    "#     },\n",
    "#     {\n",
    "#         \"transitions\": trans_num,\n",
    "#         \"eps\": 0.2,\n",
    "#         \"gamma\": 0.999,\n",
    "#         \"tau\": 0.001,\n",
    "#         \"actor_lr\": 0.0001,\n",
    "#         \"critic_lr\": 0.0002,\n",
    "#         \"sigma\": 2,\n",
    "#         \"c\": 2,\n",
    "#         \"updates_number\": 1,\n",
    "#         \"policy_delay\": 2,\n",
    "#         \"batch_size\": 256,\n",
    "#         \"start_training\": 20000\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logs = Parallel(n_jobs=6)(drun(**cfg) for cfg in tqdm(to_check))\n",
    "# for log in logs:\n",
    "#     plot(log)"
   ]
  }
 ]
}