{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "# Don't forget to install PyBullet!\n",
    "from gym import make\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(log):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    cfg = (f'alpha: {log[\"alpha\"][0]} | '\n",
    "           f'gamma: {log[\"gamma\"][0]} | '\n",
    "           f'eps: {log[\"eps_max\"][0]}, {log[\"eps_min\"][0]} | '\n",
    "           f'buffer: {log[\"buffer_size\"][0]} | '\n",
    "           f'batch: {log[\"batch_size\"][0]} | '\n",
    "           f'tau: {log[\"tau\"][0]}')\n",
    "    ax.set_title(f\"Mean reward over 10 episodes @ {cfg}\")\n",
    "    ax.set_xlabel(\"â„– of transitions\")\n",
    "    ax.set_ylabel(\"Mean reward\")\n",
    "\n",
    "    plt.hlines(200, np.min(log[\"step\"]), np.max(log[\"step\"]),\n",
    "               colors=\"r\", label=\"Solved\")\n",
    "\n",
    "    plt.plot(log[\"step\"],\n",
    "             log[\"reward_mean\"],\n",
    "             label=\"DQN\")\n",
    "\n",
    "    plt.fill_between(log[\"step\"],\n",
    "                     log[\"reward_mean\"] - log[\"reward_std\"],\n",
    "                     log[\"reward_mean\"] + log[\"reward_std\"], alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def collect_data(alpha, gamma, eps_max, eps_min,\n",
    "                 start_training,\n",
    "                 buffer_size,\n",
    "                 batch_size,\n",
    "                 hidden_size,\n",
    "                 tau):\n",
    "    env = make(\"LunarLander-v2\")\n",
    "    agent = DQN(env=env,\n",
    "                state_dim=env.observation_space.shape[0], \n",
    "                action_dim=env.action_space.n, \n",
    "                alpha=alpha, \n",
    "                gamma=gamma, \n",
    "                hidden_size=hidden_size, \n",
    "                tau=tau)\n",
    "    \n",
    "    return agent, pd.DataFrame(agent.train(1_000_000,\n",
    "                                           buffer_size=buffer_size,\n",
    "                                           batch_size=batch_size,\n",
    "                                           eps_max=eps_max,\n",
    "                                           eps_min=eps_min,\n",
    "                                           start_training=start_training\n",
    "                                           ))\n",
    "\n",
    "def print_info(log):\n",
    "    r_max_idx = np.argmax(log[\"reward_mean\"])\n",
    "    print(f\"Max mean reward {log['reward_mean'][r_max_idx]} @ {log['step'][r_max_idx]}\")"
   ]
  }
 ]
}