{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd09c458a71075000d2fe4630d72213dcf4b70214259f387c704137e873e88a8289",
   "display_name": "Python 3.8.8 64-bit ('ml': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "9c458a71075000d2fe4630d72213dcf4b70214259f387c704137e873e88a8289"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "# Don't forget to install PyBullet!\n",
    "from gym import make\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "from itertools import product\n",
    "import joblib\n",
    "from os import mkdir\n",
    "import uuid\n",
    "from train import *\n",
    "import json\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(iterations: int = 10000, \n",
    "        min_episodes_per_update: int = 4, \n",
    "        min_transitions_per_update: int = 2048,\n",
    "        actor_lr: float = 3e-4,\n",
    "        critic_lr: float = 2e-4,\n",
    "        lam: float = 0.97,\n",
    "        gamma: float = 0.99,\n",
    "        clip: float = 0.2,\n",
    "        entropy_coef: float = 1e-2,\n",
    "        batches_per_update: int = 4,\n",
    "        batch_size: int = 64\n",
    "        ):\n",
    "    working_dir = \"experiments/\" + str(uuid.uuid4())\n",
    "    os.mkdir(working_dir)\n",
    "    with open(f\"{working_dir}/params.json\", \"w\") as param:\n",
    "        json.dump({\n",
    "            \"iterations\": iterations,\n",
    "            \"min_episodes_per_update\": min_episodes_per_update,\n",
    "            \"min_transitions_per_update\": min_transitions_per_update,\n",
    "            \"actor_lr\": actor_lr,\n",
    "            \"critic_lr\": critic_lr,\n",
    "            \"lam\": lam,\n",
    "            \"gamma\": gamma,\n",
    "            \"clip\": clip,\n",
    "            \"entropy_coef\": entropy_coef,\n",
    "            \"batches_per_update\": batches_per_update,\n",
    "            \"batch_size\": batch_size\n",
    "        }, param, indent=4)\n",
    "\n",
    "    log = open(f\"{working_dir}/log.csv\", \"a+\")\n",
    "\n",
    "    env = make(\"Walker2DBulletEnv-v0\")\n",
    "    ppo = PPO(state_dim=env.observation_space.shape[0], \n",
    "              action_dim=env.action_space.shape[0], \n",
    "              actor_lr=actor_lr, \n",
    "              critic_lr=critic_lr)\n",
    "    state = env.reset()\n",
    "    episodes_sampled = 0\n",
    "    steps_sampled = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        trajectories = []\n",
    "        steps_ctn = 0\n",
    "        \n",
    "        while len(trajectories) < min_episodes_per_update or steps_ctn < min_transitions_per_update:\n",
    "            traj = sample_episode(env, ppo, lam=lam, gamma=gamma)\n",
    "            steps_ctn += len(traj)\n",
    "            trajectories.append(traj)\n",
    "        episodes_sampled += len(trajectories)\n",
    "        steps_sampled += steps_ctn\n",
    "\n",
    "        ppo.update(trajectories,\n",
    "                   clip=clip, \n",
    "                   entropy_coef=entropy_coef, \n",
    "                   batches_per_update=batches_per_update, \n",
    "                   batch_size=batch_size)\n",
    "        \n",
    "        if (i + 1) % (iterations // 100) == 0:\n",
    "            rewards = evaluate_policy(env, ppo, 50)\n",
    "            rmean = np.mean(rewards)\n",
    "            rstd = np.std(rewards)\n",
    "            log.write(f\"Rmean: {rmean:0.4f}, Rstd: {rstd:0.4f}, Episodes: {episodes_sampled}, Steps: {steps_sampled}\\n\")\n",
    "            ppo.save(name=f\"{working_dir}/{i + 1}_{int(rmean)}_{int(rstd)}.pkl\")\n",
    "    log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drun = delayed(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"iterations\": 10000,\n",
    "    \"min_episodes_per_update\": 4,\n",
    "    \"min_transitions_per_update\": 2048,\n",
    "    \"actor_lr\": 0.0003,\n",
    "    \"critic_lr\": 0.0002,\n",
    "    \"lam\": 0.97,\n",
    "    \"gamma\": 0.99,\n",
    "    \"clip\": 0.2,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"batches_per_update\": 64,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "config1 = base_config.copy()\n",
    "config2 = base_config.copy()\n",
    "config3 = base_config.copy()\n",
    "config4 = base_config.copy()\n",
    "config5 = base_config.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config1[\"entropy_coef\"] = 0\n",
    "config2[\"lam\"] = 0.99\n",
    "config3[\"clip\"] = 0.01\n",
    "config4[\"batch_size\"] = 64\n",
    "config5[\"min_episodes_per_update\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_lst = [\n",
    "    config1,\n",
    "    config2,\n",
    "    config3,\n",
    "    config4,\n",
    "    config5,\n",
    "    base_config\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parallel(n_jobs = 6)(drun(**config) for config in config_lst)"
   ]
  }
 ]
}