{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd071f4a8a05e914619e6ce849cc212b7860cf82baee4b36c2033f2d6275d70daac",
   "display_name": "Python 3.8.5 64-bit ('ml': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "PRINT_EVERY = NUM_EPOCHS // 100 if NUM_EPOCHS > 100 else 1\n",
    "TEACHER_PATH = \"./teacher.pth\"\n",
    "LR = 0.01\n",
    "NUM_WORKERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(net, loader):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    net.train()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=NUM_WORKERS)\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         shuffle=False, \n",
    "                                         num_workers=NUM_WORKERS)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = models.vgg16_bn(pretrained=True)\n",
    "teacher.classifier[6] = nn.Linear(4096,10)\n",
    "optimizer = optim.SGD(teacher.parameters(), lr=LR, momentum=0.9)\n",
    "# optimizer = optim.Adam(teacher.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded saved teacher model\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(TEACHER_PATH):\n",
    "    t = tqdm(range(NUM_EPOCHS))\n",
    "    teacher.to(DEVICE)\n",
    "    for epoch in t:\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = teacher(inputs)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss\n",
    "\n",
    "        if (epoch + 1) % PRINT_EVERY == 0:\n",
    "            acc = get_acc(teacher, testloader)\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.9f} | accuracy: {acc:0.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    torch.save(teacher.state_dict(), TEACHER_PATH)\n",
    "else:\n",
    "    print(\"Loaded saved teacher model\")\n",
    "    teacher.load_state_dict(torch.load(TEACHER_PATH))\n",
    "    teacher.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "134309962\n"
     ]
    }
   ],
   "source": [
    "TEACHER_NUM_PARAMS = sum(p.numel() for p in teacher.parameters())\n",
    "print(TEACHER_NUM_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 90.03 %\n"
     ]
    }
   ],
   "source": [
    "TEACHER_ACC = get_acc(teacher, testloader)\n",
    "print(f\"Accuracy: {TEACHER_ACC} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy for class plane is: 93.5\nAccuracy for class car is: 95.2\nAccuracy for class bird is: 87.0\nAccuracy for class cat is: 79.0\nAccuracy for class deer is: 89.8\nAccuracy for class dog is: 82.9\nAccuracy for class frog is: 93.3\nAccuracy for class horse is: 92.2\nAccuracy for class ship is: 94.7\nAccuracy for class truck is: 92.7\n"
     ]
    }
   ],
   "source": [
    "teacher.eval()\n",
    "\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data    \n",
    "        outputs = teacher(images.to(DEVICE))    \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for label, prediction in zip(labels, predictions.cpu()):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "  \n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f\"Accuracy for class {classname} is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dVGG(nn.Module):\n",
    "    def __init__(self, a=0, kind=1):\n",
    "        super().__init__()\n",
    "        self.one = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(16 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        self.two = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(16 * 10 * 10, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        self.three = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(512),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "        \n",
    "        self.a = a\n",
    "        self.kind = kind\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.kind == 1:\n",
    "            out = self.one(x)\n",
    "        elif self.kind == 2:\n",
    "            out = self.two(x)\n",
    "        elif self.kind == 3:\n",
    "            out = self.three(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected `kind`\")\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def loss(self, output, teacher_prob, real_label):\n",
    "        return self.a * F.cross_entropy(output, real_label) + (1 - self.a) * F.mse_loss(output, teacher_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(kind=3, opt=\"sgd\", coef=1):\n",
    "    print(f\"=== BASELINE: {kind} | {opt} ===\")\n",
    "    net = dVGG(1, kind).to(DEVICE)\n",
    "    \n",
    "    if opt.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "    if opt.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    if not path.exists(f\"baseline_{kind}_{opt}.pth\"):\n",
    "        for epoch in tqdm(range(int(NUM_EPOCHS * coef))):\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss\n",
    "            if (epoch + 1) % PRINT_EVERY == 0:\n",
    "                acc = get_acc(net, testloader)\n",
    "                print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.5f} | accuracy: {acc:0.2f}%')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        torch.save(net.state_dict(), f\"./baseline_{kind}_{opt}.pth\")\n",
    "        print(f\"=== Finished baseline: {kind} | {opt} ===\")\n",
    "    else:\n",
    "        print(\"Loaded saved baseline model\")\n",
    "        net.load_state_dict(torch.load(f\"baseline_{kind}_{opt}.pth\"))\n",
    "        net.to(DEVICE)\n",
    "\n",
    "    baseline_acc = get_acc(net, testloader)\n",
    "\n",
    "    print(\"Baseline accuracy on test:\", baseline_acc, \"%\")\n",
    "    return baseline_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distil(a=0, kind=3, opt=\"sgd\", coef=1):\n",
    "    print(f\"=== DISTILLATION: {a} | {kind} | {opt} ===\")\n",
    "    net = dVGG(a, kind).to(DEVICE)\n",
    "    if opt.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "    if opt.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in tqdm(range(int(NUM_EPOCHS * coef))):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs_teacher = teacher(inputs).detach()\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = net.loss(outputs, outputs_teacher, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        if (epoch + 1) % PRINT_EVERY == 0:\n",
    "            acc = get_acc(net, testloader)\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.5f} | accuracy: {acc:0.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    learner_acc = get_acc(net, testloader)\n",
    "    torch.save(net.state_dict(), f\"./distilled_{a}_{kind}_{opt}.pth\")\n",
    "\n",
    "    learner_num_params = sum(p.numel() for p in net.parameters())\n",
    "    print(f\"=== Finished distillation: {a} | {kind} | {opt} ===\")\n",
    "    print(\"\\tTotal number of teacher params:\", TEACHER_NUM_PARAMS)\n",
    "    print(\"\\tTotal number of learner params:\", learner_num_params)\n",
    "    print(\"\\tTotal reduction:\", (TEACHER_NUM_PARAMS - learner_num_params) / TEACHER_NUM_PARAMS * 100, \"%\")\n",
    "    print(\"\\tTeacher  accuracy on test:\", TEACHER_ACC, \"%\")\n",
    "    print(\"\\tLearner  accuracy on test:\", learner_acc, \"%\")\n",
    "    print(\"\\tBaseline accuracy on test:\", BASELINE_ACC, \"%\")\n",
    "    print(\"\\tDiff:\", TEACHER_ACC - learner_acc, learner_acc - BASELINE_ACC)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== BASELINE: 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18d1be07460a451cb5461888e6451af6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 2.01275 | accuracy: 16.48%\n",
      "[2] loss: 1.59383 | accuracy: 29.49%\n",
      "[3] loss: 1.42007 | accuracy: 37.44%\n",
      "[4] loss: 1.30427 | accuracy: 44.47%\n",
      "[5] loss: 1.20836 | accuracy: 48.16%\n",
      "[6] loss: 1.12366 | accuracy: 54.34%\n",
      "[7] loss: 1.05881 | accuracy: 56.25%\n",
      "[8] loss: 0.99425 | accuracy: 61.32%\n",
      "[9] loss: 0.93964 | accuracy: 62.33%\n",
      "[10] loss: 0.89668 | accuracy: 67.50%\n",
      "[11] loss: 0.85565 | accuracy: 65.71%\n",
      "[12] loss: 0.81412 | accuracy: 66.85%\n",
      "[13] loss: 0.78572 | accuracy: 70.37%\n",
      "[14] loss: 0.74874 | accuracy: 71.26%\n",
      "[15] loss: 0.72256 | accuracy: 71.29%\n",
      "[16] loss: 0.69671 | accuracy: 73.64%\n",
      "[17] loss: 0.67401 | accuracy: 74.53%\n",
      "[18] loss: 0.65573 | accuracy: 75.20%\n",
      "[19] loss: 0.62987 | accuracy: 76.40%\n",
      "[20] loss: 0.60707 | accuracy: 75.35%\n",
      "[21] loss: 0.59124 | accuracy: 76.19%\n",
      "[22] loss: 0.56987 | accuracy: 76.45%\n",
      "[23] loss: 0.55252 | accuracy: 79.69%\n",
      "[24] loss: 0.53801 | accuracy: 77.94%\n",
      "[25] loss: 0.51728 | accuracy: 78.76%\n",
      "[26] loss: 0.50479 | accuracy: 80.01%\n",
      "[27] loss: 0.48648 | accuracy: 80.01%\n",
      "[28] loss: 0.46890 | accuracy: 81.14%\n",
      "[29] loss: 0.45354 | accuracy: 80.18%\n",
      "[30] loss: 0.44109 | accuracy: 81.51%\n",
      "[31] loss: 0.42726 | accuracy: 81.29%\n",
      "[32] loss: 0.41788 | accuracy: 81.95%\n",
      "[33] loss: 0.40819 | accuracy: 82.75%\n",
      "[34] loss: 0.39058 | accuracy: 81.88%\n",
      "[35] loss: 0.37621 | accuracy: 82.49%\n",
      "[36] loss: 0.36613 | accuracy: 82.91%\n",
      "[37] loss: 0.35505 | accuracy: 83.54%\n",
      "[38] loss: 0.34470 | accuracy: 82.74%\n",
      "[39] loss: 0.33702 | accuracy: 83.09%\n",
      "[40] loss: 0.32388 | accuracy: 83.77%\n",
      "[41] loss: 0.30969 | accuracy: 83.54%\n",
      "[42] loss: 0.30059 | accuracy: 83.87%\n",
      "[43] loss: 0.29471 | accuracy: 84.17%\n",
      "[44] loss: 0.28457 | accuracy: 83.81%\n",
      "[45] loss: 0.27809 | accuracy: 83.95%\n",
      "[46] loss: 0.26189 | accuracy: 84.44%\n",
      "[47] loss: 0.25485 | accuracy: 84.46%\n",
      "[48] loss: 0.25261 | accuracy: 83.81%\n",
      "[49] loss: 0.24873 | accuracy: 84.56%\n",
      "[50] loss: 0.23552 | accuracy: 84.34%\n",
      "[51] loss: 0.22735 | accuracy: 84.53%\n",
      "[52] loss: 0.22403 | accuracy: 84.93%\n",
      "[53] loss: 0.20946 | accuracy: 84.98%\n",
      "[54] loss: 0.20977 | accuracy: 84.49%\n",
      "[55] loss: 0.20592 | accuracy: 85.49%\n",
      "[56] loss: 0.19477 | accuracy: 84.83%\n",
      "[57] loss: 0.19173 | accuracy: 84.87%\n",
      "[58] loss: 0.18567 | accuracy: 85.01%\n",
      "[59] loss: 0.17944 | accuracy: 84.86%\n",
      "[60] loss: 0.17496 | accuracy: 84.89%\n",
      "[61] loss: 0.17088 | accuracy: 85.38%\n",
      "[62] loss: 0.16480 | accuracy: 85.21%\n",
      "[63] loss: 0.16520 | accuracy: 84.92%\n",
      "[64] loss: 0.15801 | accuracy: 85.33%\n",
      "[65] loss: 0.15009 | accuracy: 85.23%\n",
      "[66] loss: 0.14942 | accuracy: 85.48%\n",
      "[67] loss: 0.14635 | accuracy: 84.86%\n",
      "[68] loss: 0.14138 | accuracy: 85.34%\n",
      "[69] loss: 0.14095 | accuracy: 85.29%\n",
      "[70] loss: 0.13543 | accuracy: 85.38%\n",
      "[71] loss: 0.13066 | accuracy: 85.42%\n",
      "[72] loss: 0.12959 | accuracy: 85.50%\n",
      "[73] loss: 0.12549 | accuracy: 85.41%\n",
      "[74] loss: 0.12342 | accuracy: 85.83%\n",
      "[75] loss: 0.12093 | accuracy: 85.83%\n",
      "[76] loss: 0.11680 | accuracy: 85.58%\n",
      "[77] loss: 0.11618 | accuracy: 86.07%\n",
      "[78] loss: 0.11499 | accuracy: 86.12%\n",
      "[79] loss: 0.11145 | accuracy: 85.45%\n",
      "[80] loss: 0.10970 | accuracy: 85.49%\n",
      "[81] loss: 0.10299 | accuracy: 85.45%\n",
      "[82] loss: 0.10356 | accuracy: 85.64%\n",
      "[83] loss: 0.09917 | accuracy: 85.89%\n",
      "[84] loss: 0.09886 | accuracy: 85.54%\n",
      "[85] loss: 0.09757 | accuracy: 85.81%\n",
      "[86] loss: 0.10062 | accuracy: 85.55%\n",
      "[87] loss: 0.09509 | accuracy: 85.51%\n",
      "[88] loss: 0.08973 | accuracy: 85.99%\n",
      "[89] loss: 0.09117 | accuracy: 86.09%\n",
      "[90] loss: 0.08898 | accuracy: 86.22%\n",
      "[91] loss: 0.08858 | accuracy: 86.07%\n",
      "[92] loss: 0.08527 | accuracy: 85.82%\n",
      "[93] loss: 0.08698 | accuracy: 85.79%\n",
      "[94] loss: 0.08513 | accuracy: 85.88%\n",
      "[95] loss: 0.08272 | accuracy: 85.93%\n",
      "[96] loss: 0.07920 | accuracy: 85.57%\n",
      "[97] loss: 0.07949 | accuracy: 86.08%\n",
      "[98] loss: 0.07686 | accuracy: 86.03%\n",
      "[99] loss: 0.07311 | accuracy: 86.16%\n",
      "[100] loss: 0.07511 | accuracy: 86.16%\n",
      "=== Finished baseline: 3 | sgd ===\n",
      "Baseline accuracy on test: 86.16 %\n",
      "=== DISTILLATION: 0 | 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd6f274886a34591869dcb8ce54f9d2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 84.23420 | accuracy: 51.48%\n",
      "[2] loss: 58.95257 | accuracy: 63.55%\n",
      "[3] loss: 47.30177 | accuracy: 71.28%\n",
      "[4] loss: 40.34689 | accuracy: 75.93%\n",
      "[5] loss: 35.76577 | accuracy: 78.65%\n",
      "[6] loss: 32.77897 | accuracy: 80.17%\n",
      "[7] loss: 30.50218 | accuracy: 81.87%\n",
      "[8] loss: 28.60887 | accuracy: 82.18%\n",
      "[9] loss: 27.23889 | accuracy: 82.33%\n",
      "[10] loss: 25.88178 | accuracy: 82.88%\n",
      "[11] loss: 24.57196 | accuracy: 83.54%\n",
      "[12] loss: 23.63063 | accuracy: 83.67%\n",
      "[13] loss: 22.68958 | accuracy: 83.71%\n",
      "[14] loss: 21.90459 | accuracy: 84.49%\n",
      "[15] loss: 21.35066 | accuracy: 84.24%\n",
      "[16] loss: 20.64750 | accuracy: 84.71%\n",
      "[17] loss: 20.00450 | accuracy: 84.55%\n",
      "[18] loss: 19.50154 | accuracy: 84.78%\n",
      "[19] loss: 19.05334 | accuracy: 85.00%\n",
      "[20] loss: 18.60837 | accuracy: 85.30%\n",
      "[21] loss: 18.22881 | accuracy: 85.71%\n",
      "[22] loss: 17.79391 | accuracy: 84.58%\n",
      "[23] loss: 17.49789 | accuracy: 85.38%\n",
      "[24] loss: 17.25982 | accuracy: 85.23%\n",
      "[25] loss: 16.61862 | accuracy: 85.57%\n",
      "[26] loss: 16.47731 | accuracy: 85.88%\n",
      "[27] loss: 16.44727 | accuracy: 85.37%\n",
      "[28] loss: 16.20001 | accuracy: 86.09%\n",
      "[29] loss: 15.97368 | accuracy: 86.22%\n",
      "[30] loss: 15.53102 | accuracy: 86.16%\n",
      "[31] loss: 15.47090 | accuracy: 85.41%\n",
      "[32] loss: 15.34305 | accuracy: 85.82%\n",
      "[33] loss: 15.10191 | accuracy: 86.19%\n",
      "[34] loss: 15.01635 | accuracy: 86.11%\n",
      "[35] loss: 14.70993 | accuracy: 86.10%\n",
      "[36] loss: 14.62795 | accuracy: 85.77%\n",
      "[37] loss: 14.55889 | accuracy: 85.57%\n",
      "[38] loss: 14.50129 | accuracy: 85.91%\n",
      "[39] loss: 14.18440 | accuracy: 85.89%\n",
      "[40] loss: 14.19271 | accuracy: 86.00%\n",
      "[41] loss: 14.07481 | accuracy: 85.77%\n",
      "[42] loss: 13.82438 | accuracy: 85.72%\n",
      "[43] loss: 13.81150 | accuracy: 86.13%\n",
      "[44] loss: 13.68214 | accuracy: 86.28%\n",
      "[45] loss: 13.73211 | accuracy: 86.50%\n",
      "[46] loss: 13.47813 | accuracy: 86.06%\n",
      "[47] loss: 13.34786 | accuracy: 85.61%\n",
      "[48] loss: 13.43863 | accuracy: 86.03%\n",
      "[49] loss: 13.38903 | accuracy: 86.68%\n",
      "[50] loss: 13.16987 | accuracy: 86.56%\n",
      "[51] loss: 13.08524 | accuracy: 86.48%\n",
      "[52] loss: 13.30426 | accuracy: 86.47%\n",
      "[53] loss: 12.84020 | accuracy: 86.15%\n",
      "[54] loss: 12.90808 | accuracy: 86.38%\n",
      "[55] loss: 12.85934 | accuracy: 86.55%\n",
      "[56] loss: 12.77174 | accuracy: 86.56%\n",
      "[57] loss: 12.60118 | accuracy: 86.54%\n",
      "[58] loss: 12.72148 | accuracy: 86.60%\n",
      "[59] loss: 12.62109 | accuracy: 86.53%\n",
      "[60] loss: 12.62297 | accuracy: 86.42%\n",
      "[61] loss: 12.54740 | accuracy: 86.11%\n",
      "[62] loss: 12.44784 | accuracy: 86.70%\n",
      "[63] loss: 12.31431 | accuracy: 86.68%\n",
      "[64] loss: 12.29881 | accuracy: 86.57%\n",
      "[65] loss: 12.34566 | accuracy: 86.56%\n",
      "[66] loss: 12.30031 | accuracy: 86.70%\n",
      "[67] loss: 12.20632 | accuracy: 86.85%\n",
      "[68] loss: 12.22738 | accuracy: 86.56%\n",
      "[69] loss: 12.09196 | accuracy: 86.94%\n",
      "[70] loss: 12.16805 | accuracy: 86.66%\n",
      "[71] loss: 12.00144 | accuracy: 86.74%\n",
      "[72] loss: 12.02787 | accuracy: 87.00%\n",
      "[73] loss: 12.00001 | accuracy: 86.50%\n",
      "[74] loss: 11.86087 | accuracy: 86.91%\n",
      "[75] loss: 11.86808 | accuracy: 86.17%\n",
      "[76] loss: 11.87527 | accuracy: 86.72%\n",
      "[77] loss: 11.88771 | accuracy: 86.69%\n",
      "[78] loss: 11.79205 | accuracy: 87.00%\n",
      "[79] loss: 11.77095 | accuracy: 86.55%\n",
      "[80] loss: 11.73889 | accuracy: 87.01%\n",
      "[81] loss: 11.63051 | accuracy: 86.73%\n",
      "[82] loss: 11.67921 | accuracy: 86.81%\n",
      "[83] loss: 11.65870 | accuracy: 86.93%\n",
      "[84] loss: 11.54867 | accuracy: 86.74%\n",
      "[85] loss: 11.70543 | accuracy: 86.74%\n",
      "[86] loss: 11.53985 | accuracy: 86.75%\n",
      "[87] loss: 11.55113 | accuracy: 87.03%\n",
      "[88] loss: 11.43156 | accuracy: 86.91%\n",
      "[89] loss: 11.40650 | accuracy: 86.83%\n",
      "[90] loss: 11.51644 | accuracy: 86.66%\n",
      "[91] loss: 11.31388 | accuracy: 87.01%\n",
      "[92] loss: 11.39581 | accuracy: 86.63%\n",
      "[93] loss: 11.38297 | accuracy: 86.81%\n",
      "[94] loss: 11.39029 | accuracy: 86.63%\n",
      "[95] loss: 11.32627 | accuracy: 86.56%\n",
      "[96] loss: 11.40325 | accuracy: 86.58%\n",
      "[97] loss: 11.43759 | accuracy: 87.06%\n",
      "[98] loss: 11.15604 | accuracy: 86.61%\n",
      "[99] loss: 11.24281 | accuracy: 86.84%\n",
      "[100] loss: 11.22641 | accuracy: 86.63%\n",
      "=== Finished distillation: 0 | 3 | sgd ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.63 %\n",
      "\tBaseline accuracy on test: 86.16 %\n",
      "\tDiff: 3.4000000000000057 0.46999999999999886\n",
      "\n",
      "=== DISTILLATION: 0.1 | 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "338271b8cd6646d4b03be0ad1d432bb7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 77.41477 | accuracy: 48.97%\n",
      "[2] loss: 53.78115 | accuracy: 64.41%\n",
      "[3] loss: 42.88178 | accuracy: 71.26%\n",
      "[4] loss: 36.57633 | accuracy: 75.72%\n",
      "[5] loss: 32.80844 | accuracy: 78.39%\n",
      "[6] loss: 29.78693 | accuracy: 80.00%\n",
      "[7] loss: 27.66233 | accuracy: 80.79%\n",
      "[8] loss: 26.03897 | accuracy: 81.52%\n",
      "[9] loss: 24.55875 | accuracy: 81.32%\n",
      "[10] loss: 23.15610 | accuracy: 83.01%\n",
      "[11] loss: 22.55224 | accuracy: 83.46%\n",
      "[12] loss: 21.64624 | accuracy: 83.75%\n",
      "[13] loss: 20.81496 | accuracy: 83.42%\n",
      "[14] loss: 19.98258 | accuracy: 83.35%\n",
      "[15] loss: 19.41748 | accuracy: 84.23%\n",
      "[16] loss: 18.82380 | accuracy: 84.58%\n",
      "[17] loss: 18.23334 | accuracy: 84.28%\n",
      "[18] loss: 17.67075 | accuracy: 84.78%\n",
      "[19] loss: 17.24389 | accuracy: 85.18%\n",
      "[20] loss: 16.74576 | accuracy: 85.21%\n",
      "[21] loss: 16.55995 | accuracy: 85.60%\n",
      "[22] loss: 16.10880 | accuracy: 85.60%\n",
      "[23] loss: 15.70543 | accuracy: 85.50%\n",
      "[24] loss: 15.56678 | accuracy: 84.87%\n",
      "[25] loss: 15.32994 | accuracy: 85.83%\n",
      "[26] loss: 15.14182 | accuracy: 85.40%\n",
      "[27] loss: 14.74927 | accuracy: 85.58%\n",
      "[28] loss: 14.52428 | accuracy: 85.63%\n",
      "[29] loss: 14.34631 | accuracy: 86.21%\n",
      "[30] loss: 14.18928 | accuracy: 86.23%\n",
      "[31] loss: 13.97316 | accuracy: 86.12%\n",
      "[32] loss: 13.84541 | accuracy: 85.97%\n",
      "[33] loss: 13.44759 | accuracy: 86.39%\n",
      "[34] loss: 13.36251 | accuracy: 86.24%\n",
      "[35] loss: 13.31987 | accuracy: 85.87%\n",
      "[36] loss: 13.29359 | accuracy: 86.13%\n",
      "[37] loss: 13.08236 | accuracy: 86.17%\n",
      "[38] loss: 12.88407 | accuracy: 85.97%\n",
      "[39] loss: 12.76192 | accuracy: 86.18%\n",
      "[40] loss: 12.71516 | accuracy: 86.07%\n",
      "[41] loss: 12.67166 | accuracy: 85.96%\n",
      "[42] loss: 12.66555 | accuracy: 86.37%\n",
      "[43] loss: 12.34397 | accuracy: 86.04%\n",
      "[44] loss: 12.31121 | accuracy: 86.15%\n",
      "[45] loss: 12.23554 | accuracy: 86.32%\n",
      "[46] loss: 12.15110 | accuracy: 86.28%\n",
      "[47] loss: 12.12501 | accuracy: 86.15%\n",
      "[48] loss: 12.09254 | accuracy: 86.15%\n",
      "[49] loss: 11.81926 | accuracy: 86.18%\n",
      "[50] loss: 11.77055 | accuracy: 86.28%\n",
      "[51] loss: 11.85014 | accuracy: 86.53%\n",
      "[52] loss: 11.76599 | accuracy: 86.38%\n",
      "[53] loss: 11.66286 | accuracy: 85.89%\n",
      "[54] loss: 11.57988 | accuracy: 86.59%\n",
      "[55] loss: 11.47453 | accuracy: 86.36%\n",
      "[56] loss: 11.45855 | accuracy: 86.75%\n",
      "[57] loss: 11.37983 | accuracy: 86.42%\n",
      "[58] loss: 11.20973 | accuracy: 86.46%\n",
      "[59] loss: 11.27317 | accuracy: 86.19%\n",
      "[60] loss: 11.15340 | accuracy: 86.05%\n",
      "[61] loss: 11.33262 | accuracy: 86.40%\n",
      "[62] loss: 11.07584 | accuracy: 86.30%\n",
      "[63] loss: 11.11416 | accuracy: 86.70%\n",
      "[64] loss: 11.03546 | accuracy: 86.50%\n",
      "[65] loss: 11.14219 | accuracy: 86.31%\n",
      "[66] loss: 10.93895 | accuracy: 86.62%\n",
      "[67] loss: 10.95110 | accuracy: 86.94%\n",
      "[68] loss: 10.96899 | accuracy: 86.53%\n",
      "[69] loss: 10.83713 | accuracy: 86.79%\n",
      "[70] loss: 10.92201 | accuracy: 86.58%\n",
      "[71] loss: 10.72845 | accuracy: 86.40%\n",
      "[72] loss: 10.74895 | accuracy: 86.56%\n",
      "[73] loss: 10.86135 | accuracy: 86.40%\n",
      "[74] loss: 10.74318 | accuracy: 86.50%\n",
      "[75] loss: 10.67013 | accuracy: 86.36%\n",
      "[76] loss: 10.70388 | accuracy: 86.65%\n",
      "[77] loss: 10.62484 | accuracy: 86.76%\n",
      "[78] loss: 10.58081 | accuracy: 86.68%\n",
      "[79] loss: 10.53747 | accuracy: 86.38%\n",
      "[80] loss: 10.52325 | accuracy: 86.32%\n",
      "[81] loss: 10.56179 | accuracy: 86.67%\n",
      "[82] loss: 10.44576 | accuracy: 86.59%\n",
      "[83] loss: 10.49131 | accuracy: 86.81%\n",
      "[84] loss: 10.51794 | accuracy: 86.62%\n",
      "[85] loss: 10.41057 | accuracy: 86.83%\n",
      "[86] loss: 10.35110 | accuracy: 86.47%\n",
      "[87] loss: 10.36370 | accuracy: 86.37%\n",
      "[88] loss: 10.31810 | accuracy: 86.50%\n",
      "[89] loss: 10.25680 | accuracy: 86.62%\n",
      "[90] loss: 10.22898 | accuracy: 86.65%\n",
      "[91] loss: 10.10384 | accuracy: 86.76%\n",
      "[92] loss: 10.22772 | accuracy: 86.61%\n",
      "[93] loss: 10.21240 | accuracy: 86.78%\n",
      "[94] loss: 10.12892 | accuracy: 86.75%\n",
      "[95] loss: 10.19719 | accuracy: 86.54%\n",
      "[96] loss: 10.12659 | accuracy: 86.64%\n",
      "[97] loss: 10.15763 | accuracy: 86.39%\n",
      "[98] loss: 10.13702 | accuracy: 86.64%\n",
      "[99] loss: 10.01839 | accuracy: 86.58%\n",
      "[100] loss: 10.07034 | accuracy: 86.46%\n",
      "=== Finished distillation: 0.1 | 3 | sgd ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.46 %\n",
      "\tBaseline accuracy on test: 86.16 %\n",
      "\tDiff: 3.5700000000000074 0.29999999999999716\n",
      "\n",
      "=== DISTILLATION: 0.5 | 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e59b260f322f41e985de3c799933d426"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 45.70777 | accuracy: 49.62%\n",
      "[2] loss: 32.33512 | accuracy: 63.87%\n",
      "[3] loss: 26.45718 | accuracy: 71.69%\n",
      "[4] loss: 22.05780 | accuracy: 75.78%\n",
      "[5] loss: 19.56806 | accuracy: 76.41%\n",
      "[6] loss: 17.83302 | accuracy: 78.98%\n",
      "[7] loss: 16.44975 | accuracy: 80.42%\n",
      "[8] loss: 15.43682 | accuracy: 82.10%\n",
      "[9] loss: 14.45893 | accuracy: 82.23%\n",
      "[10] loss: 13.78754 | accuracy: 83.28%\n",
      "[11] loss: 13.21378 | accuracy: 83.03%\n",
      "[12] loss: 12.64071 | accuracy: 83.05%\n",
      "[13] loss: 12.04774 | accuracy: 83.37%\n",
      "[14] loss: 11.62997 | accuracy: 84.51%\n",
      "[15] loss: 11.24252 | accuracy: 84.70%\n",
      "[16] loss: 10.87741 | accuracy: 84.60%\n",
      "[17] loss: 10.57896 | accuracy: 84.53%\n",
      "[18] loss: 10.31445 | accuracy: 85.09%\n",
      "[19] loss: 10.02449 | accuracy: 85.07%\n",
      "[20] loss: 9.73367 | accuracy: 84.66%\n",
      "[21] loss: 9.54885 | accuracy: 85.51%\n",
      "[22] loss: 9.31420 | accuracy: 85.75%\n",
      "[23] loss: 9.10376 | accuracy: 85.84%\n",
      "[24] loss: 8.93882 | accuracy: 85.81%\n",
      "[25] loss: 8.74564 | accuracy: 85.93%\n",
      "[26] loss: 8.53981 | accuracy: 85.70%\n",
      "[27] loss: 8.45578 | accuracy: 85.47%\n",
      "[28] loss: 8.32653 | accuracy: 86.06%\n",
      "[29] loss: 8.12004 | accuracy: 86.11%\n",
      "[30] loss: 8.06392 | accuracy: 85.69%\n",
      "[31] loss: 7.90404 | accuracy: 85.86%\n",
      "[32] loss: 7.81949 | accuracy: 86.00%\n",
      "[33] loss: 7.70106 | accuracy: 86.04%\n",
      "[34] loss: 7.63459 | accuracy: 86.07%\n",
      "[35] loss: 7.50264 | accuracy: 86.31%\n",
      "[36] loss: 7.41057 | accuracy: 86.02%\n",
      "[37] loss: 7.35906 | accuracy: 85.87%\n",
      "[38] loss: 7.30232 | accuracy: 86.08%\n",
      "[39] loss: 7.29892 | accuracy: 86.11%\n",
      "[40] loss: 7.18169 | accuracy: 86.44%\n",
      "[41] loss: 7.16902 | accuracy: 86.05%\n",
      "[42] loss: 7.04696 | accuracy: 86.37%\n",
      "[43] loss: 7.00506 | accuracy: 86.33%\n",
      "[44] loss: 6.93004 | accuracy: 86.19%\n",
      "[45] loss: 6.92765 | accuracy: 85.78%\n",
      "[46] loss: 6.85993 | accuracy: 85.98%\n",
      "[47] loss: 6.76812 | accuracy: 86.06%\n",
      "[48] loss: 6.75361 | accuracy: 86.27%\n",
      "[49] loss: 6.67795 | accuracy: 86.40%\n",
      "[50] loss: 6.64917 | accuracy: 86.49%\n",
      "[51] loss: 6.59359 | accuracy: 86.44%\n",
      "[52] loss: 6.52207 | accuracy: 86.19%\n",
      "[53] loss: 6.55636 | accuracy: 86.47%\n",
      "[54] loss: 6.50622 | accuracy: 86.43%\n",
      "[55] loss: 6.42521 | accuracy: 86.20%\n",
      "[56] loss: 6.44318 | accuracy: 86.49%\n",
      "[57] loss: 6.40021 | accuracy: 86.66%\n",
      "[58] loss: 6.36452 | accuracy: 86.96%\n",
      "[59] loss: 6.33234 | accuracy: 86.38%\n",
      "[60] loss: 6.31706 | accuracy: 86.29%\n",
      "[61] loss: 6.27919 | accuracy: 86.50%\n",
      "[62] loss: 6.24822 | accuracy: 86.61%\n",
      "[63] loss: 6.19024 | accuracy: 86.39%\n",
      "[64] loss: 6.16181 | accuracy: 86.80%\n",
      "[65] loss: 6.14745 | accuracy: 86.66%\n",
      "[66] loss: 6.09983 | accuracy: 87.06%\n",
      "[67] loss: 6.10061 | accuracy: 86.63%\n",
      "[68] loss: 6.06356 | accuracy: 86.66%\n",
      "[69] loss: 6.04018 | accuracy: 86.82%\n",
      "[70] loss: 6.05728 | accuracy: 86.58%\n",
      "[71] loss: 6.01437 | accuracy: 86.46%\n",
      "[72] loss: 5.99094 | accuracy: 86.54%\n",
      "[73] loss: 5.98491 | accuracy: 86.92%\n",
      "[74] loss: 5.95287 | accuracy: 86.73%\n",
      "[75] loss: 5.93312 | accuracy: 86.61%\n",
      "[76] loss: 5.97717 | accuracy: 86.52%\n",
      "[77] loss: 5.92481 | accuracy: 86.72%\n",
      "[78] loss: 5.89797 | accuracy: 86.65%\n",
      "[79] loss: 5.85833 | accuracy: 86.75%\n",
      "[80] loss: 5.84715 | accuracy: 86.63%\n",
      "[81] loss: 5.77238 | accuracy: 86.91%\n",
      "[82] loss: 5.78702 | accuracy: 86.33%\n",
      "[83] loss: 5.80899 | accuracy: 86.63%\n",
      "[84] loss: 5.73640 | accuracy: 87.24%\n",
      "[85] loss: 5.74966 | accuracy: 86.69%\n",
      "[86] loss: 5.73758 | accuracy: 86.38%\n",
      "[87] loss: 5.73493 | accuracy: 86.79%\n",
      "[88] loss: 5.74139 | accuracy: 86.70%\n",
      "[89] loss: 5.70690 | accuracy: 86.83%\n",
      "[90] loss: 5.68488 | accuracy: 86.77%\n",
      "[91] loss: 5.65848 | accuracy: 86.72%\n",
      "[92] loss: 5.67194 | accuracy: 86.64%\n",
      "[93] loss: 5.70277 | accuracy: 86.73%\n",
      "[94] loss: 5.65640 | accuracy: 86.84%\n",
      "[95] loss: 5.63292 | accuracy: 86.69%\n",
      "[96] loss: 5.59902 | accuracy: 86.83%\n",
      "[97] loss: 5.62787 | accuracy: 87.00%\n",
      "[98] loss: 5.57870 | accuracy: 86.77%\n",
      "[99] loss: 5.59163 | accuracy: 87.06%\n",
      "[100] loss: 5.55584 | accuracy: 86.83%\n",
      "=== Finished distillation: 0.5 | 3 | sgd ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.83 %\n",
      "\tBaseline accuracy on test: 86.16 %\n",
      "\tDiff: 3.200000000000003 0.6700000000000017\n",
      "\n",
      "=== DISTILLATION: 0.7 | 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b59ca118b5a498998cf4162f7c6bbb7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 29.44281 | accuracy: 42.69%\n",
      "[2] loss: 21.64263 | accuracy: 57.17%\n",
      "[3] loss: 17.52663 | accuracy: 65.95%\n",
      "[4] loss: 15.08388 | accuracy: 71.92%\n",
      "[5] loss: 13.28201 | accuracy: 76.07%\n",
      "[6] loss: 12.06145 | accuracy: 78.80%\n",
      "[7] loss: 11.05220 | accuracy: 80.18%\n",
      "[8] loss: 10.40441 | accuracy: 79.93%\n",
      "[9] loss: 9.69715 | accuracy: 82.44%\n",
      "[10] loss: 9.17740 | accuracy: 82.65%\n",
      "[11] loss: 8.72670 | accuracy: 83.60%\n",
      "[12] loss: 8.29822 | accuracy: 84.00%\n",
      "[13] loss: 7.96676 | accuracy: 84.17%\n",
      "[14] loss: 7.54099 | accuracy: 85.08%\n",
      "[15] loss: 7.37635 | accuracy: 84.83%\n",
      "[16] loss: 7.10875 | accuracy: 84.80%\n",
      "[17] loss: 6.89366 | accuracy: 85.50%\n",
      "[18] loss: 6.70002 | accuracy: 85.29%\n",
      "[19] loss: 6.44706 | accuracy: 85.58%\n",
      "[20] loss: 6.26881 | accuracy: 85.92%\n",
      "[21] loss: 6.09624 | accuracy: 86.03%\n",
      "[22] loss: 5.86030 | accuracy: 86.19%\n",
      "[23] loss: 5.76468 | accuracy: 85.91%\n",
      "[24] loss: 5.71878 | accuracy: 85.99%\n",
      "[25] loss: 5.54422 | accuracy: 86.14%\n",
      "[26] loss: 5.48012 | accuracy: 86.49%\n",
      "[27] loss: 5.40230 | accuracy: 86.31%\n",
      "[28] loss: 5.27808 | accuracy: 86.52%\n",
      "[29] loss: 5.14049 | accuracy: 86.54%\n",
      "[30] loss: 5.07228 | accuracy: 86.20%\n",
      "[31] loss: 4.99478 | accuracy: 86.19%\n",
      "[32] loss: 4.90363 | accuracy: 86.56%\n",
      "[33] loss: 4.88483 | accuracy: 86.26%\n",
      "[34] loss: 4.75412 | accuracy: 86.36%\n",
      "[35] loss: 4.75189 | accuracy: 86.48%\n",
      "[36] loss: 4.70301 | accuracy: 86.64%\n",
      "[37] loss: 4.65617 | accuracy: 86.68%\n",
      "[38] loss: 4.56832 | accuracy: 86.34%\n",
      "[39] loss: 4.50503 | accuracy: 87.07%\n",
      "[40] loss: 4.42212 | accuracy: 86.87%\n",
      "[41] loss: 4.42206 | accuracy: 86.72%\n",
      "[42] loss: 4.34764 | accuracy: 86.76%\n",
      "[43] loss: 4.33195 | accuracy: 86.59%\n",
      "[44] loss: 4.34353 | accuracy: 86.77%\n",
      "[45] loss: 4.25457 | accuracy: 87.24%\n",
      "[46] loss: 4.21364 | accuracy: 86.88%\n",
      "[47] loss: 4.19267 | accuracy: 87.13%\n",
      "[48] loss: 4.20778 | accuracy: 86.65%\n",
      "[49] loss: 4.14652 | accuracy: 87.00%\n",
      "[50] loss: 4.08812 | accuracy: 87.02%\n",
      "[51] loss: 4.10445 | accuracy: 87.20%\n",
      "[52] loss: 4.11440 | accuracy: 87.16%\n",
      "[53] loss: 4.00659 | accuracy: 86.99%\n",
      "[54] loss: 4.00951 | accuracy: 86.69%\n",
      "[55] loss: 3.99238 | accuracy: 87.27%\n",
      "[56] loss: 3.96835 | accuracy: 87.15%\n",
      "[57] loss: 3.93400 | accuracy: 87.12%\n",
      "[58] loss: 3.92992 | accuracy: 86.98%\n",
      "[59] loss: 3.88488 | accuracy: 87.13%\n",
      "[60] loss: 3.88280 | accuracy: 87.12%\n",
      "[61] loss: 3.84577 | accuracy: 87.00%\n",
      "[62] loss: 3.83378 | accuracy: 86.90%\n",
      "[63] loss: 3.81766 | accuracy: 87.10%\n",
      "[64] loss: 3.79910 | accuracy: 87.36%\n",
      "[65] loss: 3.75796 | accuracy: 86.87%\n",
      "[66] loss: 3.77664 | accuracy: 86.76%\n",
      "[67] loss: 3.76897 | accuracy: 87.37%\n",
      "[68] loss: 3.74682 | accuracy: 87.35%\n",
      "[69] loss: 3.69624 | accuracy: 87.34%\n",
      "[70] loss: 3.69807 | accuracy: 87.37%\n",
      "[71] loss: 3.69135 | accuracy: 87.29%\n",
      "[72] loss: 3.66265 | accuracy: 87.38%\n",
      "[73] loss: 3.64618 | accuracy: 87.21%\n",
      "[74] loss: 3.66797 | accuracy: 87.38%\n",
      "[75] loss: 3.64697 | accuracy: 87.25%\n",
      "[76] loss: 3.63010 | accuracy: 87.60%\n",
      "[77] loss: 3.61225 | accuracy: 87.13%\n",
      "[78] loss: 3.60273 | accuracy: 87.34%\n",
      "[79] loss: 3.58629 | accuracy: 87.24%\n",
      "[80] loss: 3.57566 | accuracy: 87.47%\n",
      "[81] loss: 3.59261 | accuracy: 87.52%\n",
      "[82] loss: 3.53957 | accuracy: 87.52%\n",
      "[83] loss: 3.55779 | accuracy: 87.44%\n",
      "[84] loss: 3.49432 | accuracy: 87.30%\n",
      "[85] loss: 3.51692 | accuracy: 87.40%\n",
      "[86] loss: 3.53560 | accuracy: 87.44%\n",
      "[87] loss: 3.49264 | accuracy: 87.44%\n",
      "[88] loss: 3.45611 | accuracy: 87.40%\n",
      "[89] loss: 3.51173 | accuracy: 87.42%\n",
      "[90] loss: 3.49790 | accuracy: 87.46%\n",
      "[91] loss: 3.45410 | accuracy: 87.55%\n",
      "[92] loss: 3.45568 | accuracy: 87.26%\n",
      "[93] loss: 3.41848 | accuracy: 87.79%\n",
      "[94] loss: 3.44915 | accuracy: 87.47%\n",
      "[95] loss: 3.43844 | accuracy: 87.57%\n",
      "[96] loss: 3.41305 | accuracy: 87.41%\n",
      "[97] loss: 3.40270 | accuracy: 87.36%\n",
      "[98] loss: 3.38856 | accuracy: 87.51%\n",
      "[99] loss: 3.40551 | accuracy: 87.54%\n",
      "[100] loss: 3.41751 | accuracy: 87.41%\n",
      "=== Finished distillation: 0.7 | 3 | sgd ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 87.41 %\n",
      "\tBaseline accuracy on test: 86.16 %\n",
      "\tDiff: 2.6200000000000045 1.25\n",
      "\n",
      "=== DISTILLATION: 0.9 | 3 | sgd ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ebaef8fd4be44d0b0bf9e26dee32341"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 11.78005 | accuracy: 22.24%\n",
      "[2] loss: 9.68677 | accuracy: 48.76%\n",
      "[3] loss: 8.36221 | accuracy: 56.59%\n",
      "[4] loss: 7.42693 | accuracy: 63.44%\n",
      "[5] loss: 6.66258 | accuracy: 69.73%\n",
      "[6] loss: 6.18992 | accuracy: 73.76%\n",
      "[7] loss: 5.70670 | accuracy: 76.24%\n",
      "[8] loss: 5.36232 | accuracy: 78.01%\n",
      "[9] loss: 4.98056 | accuracy: 79.40%\n",
      "[10] loss: 4.64276 | accuracy: 80.55%\n",
      "[11] loss: 4.45216 | accuracy: 81.85%\n",
      "[12] loss: 4.21181 | accuracy: 81.67%\n",
      "[13] loss: 4.01118 | accuracy: 82.53%\n",
      "[14] loss: 3.86302 | accuracy: 83.55%\n",
      "[15] loss: 3.67982 | accuracy: 83.40%\n",
      "[16] loss: 3.53470 | accuracy: 84.18%\n",
      "[17] loss: 3.38351 | accuracy: 84.38%\n",
      "[18] loss: 3.28840 | accuracy: 84.65%\n",
      "[19] loss: 3.12890 | accuracy: 84.70%\n",
      "[20] loss: 3.03602 | accuracy: 85.17%\n",
      "[21] loss: 2.94719 | accuracy: 85.05%\n",
      "[22] loss: 2.85042 | accuracy: 85.91%\n",
      "[23] loss: 2.74644 | accuracy: 85.50%\n",
      "[24] loss: 2.68239 | accuracy: 86.00%\n",
      "[25] loss: 2.61589 | accuracy: 85.80%\n",
      "[26] loss: 2.53777 | accuracy: 86.02%\n",
      "[27] loss: 2.45240 | accuracy: 85.89%\n",
      "[28] loss: 2.40843 | accuracy: 85.50%\n",
      "[29] loss: 2.36677 | accuracy: 86.10%\n",
      "[30] loss: 2.30372 | accuracy: 86.16%\n",
      "[31] loss: 2.26372 | accuracy: 86.48%\n",
      "[32] loss: 2.18656 | accuracy: 86.04%\n",
      "[33] loss: 2.17378 | accuracy: 86.36%\n",
      "[34] loss: 2.11367 | accuracy: 86.36%\n",
      "[35] loss: 2.08723 | accuracy: 86.44%\n",
      "[36] loss: 2.06238 | accuracy: 86.63%\n",
      "[37] loss: 2.03464 | accuracy: 86.22%\n",
      "[38] loss: 1.96525 | accuracy: 86.42%\n",
      "[39] loss: 1.92812 | accuracy: 86.71%\n",
      "[40] loss: 1.89507 | accuracy: 86.54%\n",
      "[41] loss: 1.88033 | accuracy: 86.92%\n",
      "[42] loss: 1.86148 | accuracy: 86.47%\n",
      "[43] loss: 1.82221 | accuracy: 86.65%\n",
      "[44] loss: 1.81479 | accuracy: 86.86%\n",
      "[45] loss: 1.81079 | accuracy: 86.56%\n",
      "[46] loss: 1.76935 | accuracy: 86.94%\n",
      "[47] loss: 1.73200 | accuracy: 86.79%\n",
      "[48] loss: 1.70795 | accuracy: 86.85%\n",
      "[49] loss: 1.70532 | accuracy: 86.69%\n",
      "[50] loss: 1.68328 | accuracy: 87.13%\n",
      "[51] loss: 1.67845 | accuracy: 87.04%\n",
      "[52] loss: 1.64333 | accuracy: 86.62%\n",
      "[53] loss: 1.62698 | accuracy: 87.11%\n",
      "[54] loss: 1.60087 | accuracy: 87.19%\n",
      "[55] loss: 1.57704 | accuracy: 87.14%\n",
      "[56] loss: 1.57982 | accuracy: 86.89%\n",
      "[57] loss: 1.57679 | accuracy: 86.98%\n",
      "[58] loss: 1.58056 | accuracy: 87.30%\n",
      "[59] loss: 1.55552 | accuracy: 87.05%\n",
      "[60] loss: 1.52856 | accuracy: 87.20%\n",
      "[61] loss: 1.53749 | accuracy: 87.05%\n",
      "[62] loss: 1.51312 | accuracy: 86.83%\n",
      "[63] loss: 1.50044 | accuracy: 86.96%\n",
      "[64] loss: 1.48989 | accuracy: 87.29%\n",
      "[65] loss: 1.48515 | accuracy: 86.93%\n",
      "[66] loss: 1.49423 | accuracy: 86.91%\n",
      "[67] loss: 1.47571 | accuracy: 87.02%\n",
      "[68] loss: 1.46125 | accuracy: 87.23%\n",
      "[69] loss: 1.44495 | accuracy: 87.20%\n",
      "[70] loss: 1.43656 | accuracy: 87.28%\n",
      "[71] loss: 1.43415 | accuracy: 87.18%\n",
      "[72] loss: 1.42764 | accuracy: 87.31%\n",
      "[73] loss: 1.41799 | accuracy: 86.94%\n",
      "[74] loss: 1.41058 | accuracy: 87.49%\n",
      "[75] loss: 1.39567 | accuracy: 87.00%\n",
      "[76] loss: 1.39966 | accuracy: 87.41%\n",
      "[77] loss: 1.38575 | accuracy: 87.25%\n",
      "[78] loss: 1.38572 | accuracy: 87.36%\n",
      "[79] loss: 1.37264 | accuracy: 87.55%\n",
      "[80] loss: 1.36838 | accuracy: 87.40%\n",
      "[81] loss: 1.36586 | accuracy: 87.50%\n",
      "[82] loss: 1.36512 | accuracy: 87.51%\n",
      "[83] loss: 1.33630 | accuracy: 87.29%\n",
      "[84] loss: 1.34545 | accuracy: 87.40%\n",
      "[85] loss: 1.32023 | accuracy: 87.19%\n",
      "[86] loss: 1.31834 | accuracy: 87.61%\n",
      "[87] loss: 1.33088 | accuracy: 87.64%\n",
      "[88] loss: 1.32685 | accuracy: 87.31%\n",
      "[89] loss: 1.32379 | accuracy: 87.24%\n",
      "[90] loss: 1.31046 | accuracy: 87.28%\n",
      "[91] loss: 1.32098 | accuracy: 87.47%\n",
      "[92] loss: 1.31012 | accuracy: 87.33%\n",
      "[93] loss: 1.30573 | accuracy: 87.43%\n",
      "[94] loss: 1.29341 | accuracy: 87.68%\n",
      "[95] loss: 1.28889 | accuracy: 87.43%\n",
      "[96] loss: 1.29237 | accuracy: 87.62%\n",
      "[97] loss: 1.27874 | accuracy: 87.34%\n",
      "[98] loss: 1.26216 | accuracy: 87.55%\n",
      "[99] loss: 1.28497 | accuracy: 87.82%\n",
      "[100] loss: 1.28180 | accuracy: 87.61%\n",
      "=== Finished distillation: 0.9 | 3 | sgd ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 87.61 %\n",
      "\tBaseline accuracy on test: 86.16 %\n",
      "\tDiff: 2.4200000000000017 1.4500000000000028\n",
      "\n",
      "=== BASELINE: 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f47377e266bd4ae9afa918ba388e89dd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 1.69565 | accuracy: 50.44%\n",
      "[2] loss: 1.28266 | accuracy: 61.07%\n",
      "[3] loss: 1.07446 | accuracy: 65.20%\n",
      "[4] loss: 0.91172 | accuracy: 74.05%\n",
      "[5] loss: 0.80041 | accuracy: 75.68%\n",
      "[6] loss: 0.73305 | accuracy: 78.38%\n",
      "[7] loss: 0.66862 | accuracy: 78.02%\n",
      "[8] loss: 0.61663 | accuracy: 78.80%\n",
      "[9] loss: 0.57885 | accuracy: 79.20%\n",
      "[10] loss: 0.55136 | accuracy: 81.20%\n",
      "[11] loss: 0.50726 | accuracy: 81.84%\n",
      "[12] loss: 0.48418 | accuracy: 81.66%\n",
      "[13] loss: 0.45599 | accuracy: 80.31%\n",
      "[14] loss: 0.43578 | accuracy: 82.81%\n",
      "[15] loss: 0.41570 | accuracy: 82.66%\n",
      "[16] loss: 0.40367 | accuracy: 82.30%\n",
      "[17] loss: 0.37368 | accuracy: 81.47%\n",
      "[18] loss: 0.36716 | accuracy: 82.82%\n",
      "[19] loss: 0.35288 | accuracy: 82.86%\n",
      "[20] loss: 0.33561 | accuracy: 83.48%\n",
      "[21] loss: 0.32482 | accuracy: 83.52%\n",
      "[22] loss: 0.30887 | accuracy: 83.26%\n",
      "[23] loss: 0.30334 | accuracy: 82.71%\n",
      "[24] loss: 0.29436 | accuracy: 84.00%\n",
      "[25] loss: 0.28168 | accuracy: 83.39%\n",
      "[26] loss: 0.27038 | accuracy: 83.23%\n",
      "[27] loss: 0.25933 | accuracy: 84.15%\n",
      "[28] loss: 0.26333 | accuracy: 84.12%\n",
      "[29] loss: 0.24997 | accuracy: 83.45%\n",
      "[30] loss: 0.24122 | accuracy: 84.27%\n",
      "[31] loss: 0.23009 | accuracy: 83.81%\n",
      "[32] loss: 0.23366 | accuracy: 83.61%\n",
      "[33] loss: 0.23415 | accuracy: 83.83%\n",
      "[34] loss: 0.23057 | accuracy: 83.93%\n",
      "[35] loss: 0.21740 | accuracy: 83.32%\n",
      "[36] loss: 0.21969 | accuracy: 84.20%\n",
      "[37] loss: 0.21018 | accuracy: 83.88%\n",
      "[38] loss: 0.20138 | accuracy: 83.17%\n",
      "[39] loss: 0.19969 | accuracy: 84.45%\n",
      "[40] loss: 0.19452 | accuracy: 83.34%\n",
      "[41] loss: 0.19655 | accuracy: 83.63%\n",
      "[42] loss: 0.18551 | accuracy: 84.16%\n",
      "[43] loss: 0.18415 | accuracy: 83.75%\n",
      "[44] loss: 0.18779 | accuracy: 83.77%\n",
      "[45] loss: 0.17873 | accuracy: 84.39%\n",
      "[46] loss: 0.17653 | accuracy: 84.22%\n",
      "[47] loss: 0.17633 | accuracy: 83.94%\n",
      "[48] loss: 0.17170 | accuracy: 82.83%\n",
      "[49] loss: 0.18663 | accuracy: 84.30%\n",
      "[50] loss: 0.17601 | accuracy: 84.03%\n",
      "[51] loss: 0.16972 | accuracy: 84.06%\n",
      "[52] loss: 0.16843 | accuracy: 83.77%\n",
      "[53] loss: 0.16198 | accuracy: 83.43%\n",
      "[54] loss: 0.16173 | accuracy: 83.66%\n",
      "[55] loss: 0.15513 | accuracy: 83.89%\n",
      "[56] loss: 0.15297 | accuracy: 84.18%\n",
      "[57] loss: 0.15809 | accuracy: 84.83%\n",
      "[58] loss: 0.15436 | accuracy: 84.49%\n",
      "[59] loss: 0.14900 | accuracy: 83.20%\n",
      "[60] loss: 0.15738 | accuracy: 84.38%\n",
      "[61] loss: 0.15650 | accuracy: 84.35%\n",
      "[62] loss: 0.14133 | accuracy: 84.38%\n",
      "[63] loss: 0.13803 | accuracy: 84.22%\n",
      "[64] loss: 0.14772 | accuracy: 84.55%\n",
      "[65] loss: 0.14252 | accuracy: 84.78%\n",
      "[66] loss: 0.14033 | accuracy: 84.55%\n",
      "[67] loss: 0.13700 | accuracy: 84.87%\n",
      "[68] loss: 0.13311 | accuracy: 84.40%\n",
      "[69] loss: 0.13710 | accuracy: 84.05%\n",
      "[70] loss: 0.13992 | accuracy: 83.89%\n",
      "[71] loss: 0.12632 | accuracy: 84.64%\n",
      "[72] loss: 0.13895 | accuracy: 84.46%\n",
      "[73] loss: 0.13046 | accuracy: 84.58%\n",
      "[74] loss: 0.13334 | accuracy: 84.36%\n",
      "[75] loss: 0.13269 | accuracy: 84.98%\n",
      "[76] loss: 0.13061 | accuracy: 84.23%\n",
      "[77] loss: 0.12176 | accuracy: 85.21%\n",
      "[78] loss: 0.12888 | accuracy: 84.37%\n",
      "[79] loss: 0.12654 | accuracy: 84.70%\n",
      "[80] loss: 0.12826 | accuracy: 85.07%\n",
      "[81] loss: 0.12996 | accuracy: 84.53%\n",
      "[82] loss: 0.12536 | accuracy: 83.87%\n",
      "[83] loss: 0.12424 | accuracy: 84.06%\n",
      "[84] loss: 0.12578 | accuracy: 84.55%\n",
      "[85] loss: 0.12577 | accuracy: 84.12%\n",
      "[86] loss: 0.12410 | accuracy: 84.74%\n",
      "[87] loss: 0.11713 | accuracy: 84.78%\n",
      "[88] loss: 0.11271 | accuracy: 84.78%\n",
      "[89] loss: 0.12598 | accuracy: 85.03%\n",
      "[90] loss: 0.12040 | accuracy: 84.35%\n",
      "[91] loss: 0.11754 | accuracy: 84.59%\n",
      "[92] loss: 0.12115 | accuracy: 84.55%\n",
      "[93] loss: 0.11915 | accuracy: 84.09%\n",
      "[94] loss: 0.11584 | accuracy: 84.72%\n",
      "[95] loss: 0.11253 | accuracy: 84.53%\n",
      "[96] loss: 0.10661 | accuracy: 83.97%\n",
      "[97] loss: 0.11295 | accuracy: 84.68%\n",
      "[98] loss: 0.11207 | accuracy: 84.64%\n",
      "[99] loss: 0.11197 | accuracy: 84.89%\n",
      "[100] loss: 0.11202 | accuracy: 84.39%\n",
      "=== Finished baseline: 3 | adam ===\n",
      "Baseline accuracy on test: 84.39 %\n",
      "=== DISTILLATION: 0 | 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8006aa37a7146dc83db1a6e07daf472"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 80.34827 | accuracy: 55.65%\n",
      "[2] loss: 56.84294 | accuracy: 62.76%\n",
      "[3] loss: 45.58154 | accuracy: 73.22%\n",
      "[4] loss: 39.27451 | accuracy: 76.99%\n",
      "[5] loss: 35.26430 | accuracy: 78.94%\n",
      "[6] loss: 32.43030 | accuracy: 80.35%\n",
      "[7] loss: 30.18510 | accuracy: 79.77%\n",
      "[8] loss: 28.49910 | accuracy: 80.57%\n",
      "[9] loss: 26.79936 | accuracy: 82.57%\n",
      "[10] loss: 25.87925 | accuracy: 81.91%\n",
      "[11] loss: 24.66003 | accuracy: 83.15%\n",
      "[12] loss: 23.95755 | accuracy: 83.65%\n",
      "[13] loss: 22.87745 | accuracy: 83.98%\n",
      "[14] loss: 22.57643 | accuracy: 84.60%\n",
      "[15] loss: 21.81855 | accuracy: 84.37%\n",
      "[16] loss: 20.74304 | accuracy: 84.27%\n",
      "[17] loss: 20.54493 | accuracy: 84.67%\n",
      "[18] loss: 20.02566 | accuracy: 84.62%\n",
      "[19] loss: 19.60288 | accuracy: 85.16%\n",
      "[20] loss: 18.88608 | accuracy: 84.39%\n",
      "[21] loss: 18.78412 | accuracy: 85.47%\n",
      "[22] loss: 18.29001 | accuracy: 84.91%\n",
      "[23] loss: 17.93477 | accuracy: 85.27%\n",
      "[24] loss: 17.64173 | accuracy: 85.31%\n",
      "[25] loss: 17.33259 | accuracy: 85.06%\n",
      "[26] loss: 17.16073 | accuracy: 85.72%\n",
      "[27] loss: 16.90819 | accuracy: 84.88%\n",
      "[28] loss: 16.75310 | accuracy: 85.08%\n",
      "[29] loss: 16.52829 | accuracy: 85.56%\n",
      "[30] loss: 16.28707 | accuracy: 85.63%\n",
      "[31] loss: 15.90923 | accuracy: 85.97%\n",
      "[32] loss: 15.91373 | accuracy: 85.33%\n",
      "[33] loss: 15.72293 | accuracy: 85.78%\n",
      "[34] loss: 15.65948 | accuracy: 85.90%\n",
      "[35] loss: 15.40876 | accuracy: 85.54%\n",
      "[36] loss: 15.35540 | accuracy: 85.49%\n",
      "[37] loss: 15.32230 | accuracy: 86.01%\n",
      "[38] loss: 15.28191 | accuracy: 85.98%\n",
      "[39] loss: 14.89807 | accuracy: 85.02%\n",
      "[40] loss: 14.92132 | accuracy: 84.90%\n",
      "[41] loss: 14.64248 | accuracy: 85.69%\n",
      "[42] loss: 14.58126 | accuracy: 86.08%\n",
      "[43] loss: 14.67967 | accuracy: 86.03%\n",
      "[44] loss: 14.72364 | accuracy: 86.06%\n",
      "[45] loss: 14.50124 | accuracy: 86.41%\n",
      "[46] loss: 14.21043 | accuracy: 86.16%\n",
      "[47] loss: 14.24189 | accuracy: 86.32%\n",
      "[48] loss: 14.02505 | accuracy: 86.28%\n",
      "[49] loss: 14.04871 | accuracy: 85.53%\n",
      "[50] loss: 14.04468 | accuracy: 85.87%\n",
      "[51] loss: 13.87949 | accuracy: 85.34%\n",
      "[52] loss: 14.27363 | accuracy: 85.57%\n",
      "[53] loss: 13.84365 | accuracy: 86.08%\n",
      "[54] loss: 13.59151 | accuracy: 86.31%\n",
      "[55] loss: 13.68282 | accuracy: 85.82%\n",
      "[56] loss: 13.58668 | accuracy: 86.13%\n",
      "[57] loss: 13.60824 | accuracy: 86.12%\n",
      "[58] loss: 13.52850 | accuracy: 86.23%\n",
      "[59] loss: 13.57970 | accuracy: 86.26%\n",
      "[60] loss: 13.34528 | accuracy: 85.65%\n",
      "[61] loss: 13.38319 | accuracy: 86.39%\n",
      "[62] loss: 13.21731 | accuracy: 86.01%\n",
      "[63] loss: 13.23343 | accuracy: 85.88%\n",
      "[64] loss: 13.23216 | accuracy: 85.88%\n",
      "[65] loss: 13.29923 | accuracy: 86.14%\n",
      "[66] loss: 13.18843 | accuracy: 86.29%\n",
      "[67] loss: 13.30782 | accuracy: 86.07%\n",
      "[68] loss: 13.05016 | accuracy: 86.19%\n",
      "[69] loss: 13.08656 | accuracy: 85.72%\n",
      "[70] loss: 12.95819 | accuracy: 85.94%\n",
      "[71] loss: 12.79247 | accuracy: 86.23%\n",
      "[72] loss: 13.04085 | accuracy: 85.44%\n",
      "[73] loss: 12.72742 | accuracy: 86.31%\n",
      "[74] loss: 12.91675 | accuracy: 85.70%\n",
      "[75] loss: 12.76517 | accuracy: 86.14%\n",
      "[76] loss: 12.71832 | accuracy: 86.16%\n",
      "[77] loss: 12.84072 | accuracy: 86.30%\n",
      "[78] loss: 12.60445 | accuracy: 86.53%\n",
      "[79] loss: 12.68456 | accuracy: 86.39%\n",
      "[80] loss: 12.52446 | accuracy: 86.30%\n",
      "[81] loss: 12.60463 | accuracy: 86.54%\n",
      "[82] loss: 12.49057 | accuracy: 86.64%\n",
      "[83] loss: 12.50021 | accuracy: 86.48%\n",
      "[84] loss: 12.56973 | accuracy: 86.69%\n",
      "[85] loss: 12.61204 | accuracy: 86.27%\n",
      "[86] loss: 12.36933 | accuracy: 86.39%\n",
      "[87] loss: 12.42833 | accuracy: 86.33%\n",
      "[88] loss: 12.24901 | accuracy: 85.97%\n",
      "[89] loss: 12.52625 | accuracy: 86.31%\n",
      "[90] loss: 12.21724 | accuracy: 86.61%\n",
      "[91] loss: 12.03971 | accuracy: 86.47%\n",
      "[92] loss: 12.23540 | accuracy: 86.47%\n",
      "[93] loss: 12.13177 | accuracy: 86.52%\n",
      "[94] loss: 12.15585 | accuracy: 86.27%\n",
      "[95] loss: 12.21149 | accuracy: 86.91%\n",
      "[96] loss: 12.25414 | accuracy: 87.09%\n",
      "[97] loss: 12.17539 | accuracy: 86.49%\n",
      "[98] loss: 12.10097 | accuracy: 86.54%\n",
      "[99] loss: 11.91044 | accuracy: 86.51%\n",
      "[100] loss: 12.04751 | accuracy: 86.67%\n",
      "=== Finished distillation: 0 | 3 | adam ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.67 %\n",
      "\tBaseline accuracy on test: 84.39 %\n",
      "\tDiff: 3.3599999999999994 2.280000000000001\n",
      "\n",
      "=== DISTILLATION: 0.1 | 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9707abd6b38b4c0f986542934e5c9e79"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 72.96763 | accuracy: 53.66%\n",
      "[2] loss: 51.61487 | accuracy: 65.74%\n",
      "[3] loss: 41.74505 | accuracy: 72.82%\n",
      "[4] loss: 35.61942 | accuracy: 75.09%\n",
      "[5] loss: 31.66616 | accuracy: 78.13%\n",
      "[6] loss: 29.33301 | accuracy: 79.34%\n",
      "[7] loss: 27.36659 | accuracy: 81.14%\n",
      "[8] loss: 25.65966 | accuracy: 82.36%\n",
      "[9] loss: 24.45773 | accuracy: 83.16%\n",
      "[10] loss: 23.53576 | accuracy: 82.88%\n",
      "[11] loss: 22.27504 | accuracy: 83.43%\n",
      "[12] loss: 21.43976 | accuracy: 82.58%\n",
      "[13] loss: 20.69879 | accuracy: 84.24%\n",
      "[14] loss: 20.00186 | accuracy: 84.53%\n",
      "[15] loss: 19.32882 | accuracy: 84.54%\n",
      "[16] loss: 19.10549 | accuracy: 84.67%\n",
      "[17] loss: 18.51427 | accuracy: 84.61%\n",
      "[18] loss: 18.03265 | accuracy: 84.76%\n",
      "[19] loss: 17.49799 | accuracy: 84.73%\n",
      "[20] loss: 17.01663 | accuracy: 85.06%\n",
      "[21] loss: 17.03398 | accuracy: 85.22%\n",
      "[22] loss: 16.42495 | accuracy: 84.85%\n",
      "[23] loss: 16.20286 | accuracy: 85.09%\n",
      "[24] loss: 15.96742 | accuracy: 85.66%\n",
      "[25] loss: 15.64665 | accuracy: 85.14%\n",
      "[26] loss: 15.42966 | accuracy: 85.38%\n",
      "[27] loss: 15.15615 | accuracy: 85.84%\n",
      "[28] loss: 15.28095 | accuracy: 85.40%\n",
      "[29] loss: 14.82195 | accuracy: 85.70%\n",
      "[30] loss: 14.69278 | accuracy: 85.18%\n",
      "[31] loss: 14.55081 | accuracy: 85.23%\n",
      "[32] loss: 14.35475 | accuracy: 85.72%\n",
      "[33] loss: 14.10046 | accuracy: 85.60%\n",
      "[34] loss: 13.92596 | accuracy: 85.56%\n",
      "[35] loss: 14.04121 | accuracy: 86.05%\n",
      "[36] loss: 13.89962 | accuracy: 85.66%\n",
      "[37] loss: 13.60420 | accuracy: 86.04%\n",
      "[38] loss: 13.53610 | accuracy: 86.44%\n",
      "[39] loss: 13.35081 | accuracy: 85.84%\n",
      "[40] loss: 13.49725 | accuracy: 86.32%\n",
      "[41] loss: 13.20733 | accuracy: 86.18%\n",
      "[42] loss: 13.27255 | accuracy: 86.08%\n",
      "[43] loss: 12.99933 | accuracy: 86.23%\n",
      "[44] loss: 13.05007 | accuracy: 86.38%\n",
      "[45] loss: 12.95703 | accuracy: 85.89%\n",
      "[46] loss: 13.02696 | accuracy: 85.97%\n",
      "[47] loss: 12.79114 | accuracy: 86.06%\n",
      "[48] loss: 12.71285 | accuracy: 85.78%\n",
      "[49] loss: 12.57664 | accuracy: 86.46%\n",
      "[50] loss: 12.66857 | accuracy: 86.36%\n",
      "[51] loss: 12.52921 | accuracy: 86.01%\n",
      "[52] loss: 12.47954 | accuracy: 85.93%\n",
      "[53] loss: 12.32741 | accuracy: 86.06%\n",
      "[54] loss: 12.40909 | accuracy: 86.07%\n",
      "[55] loss: 12.36188 | accuracy: 86.27%\n",
      "[56] loss: 12.36336 | accuracy: 86.51%\n",
      "[57] loss: 12.09709 | accuracy: 86.38%\n",
      "[58] loss: 12.10635 | accuracy: 86.40%\n",
      "[59] loss: 12.21430 | accuracy: 86.41%\n",
      "[60] loss: 11.98665 | accuracy: 86.59%\n",
      "[61] loss: 12.13316 | accuracy: 86.54%\n",
      "[62] loss: 11.82254 | accuracy: 86.60%\n",
      "[63] loss: 11.98608 | accuracy: 86.49%\n",
      "[64] loss: 11.82892 | accuracy: 86.10%\n",
      "[65] loss: 11.92341 | accuracy: 86.68%\n",
      "[66] loss: 11.90625 | accuracy: 86.82%\n",
      "[67] loss: 11.71031 | accuracy: 86.74%\n",
      "[68] loss: 11.80351 | accuracy: 86.14%\n",
      "[69] loss: 11.66608 | accuracy: 86.59%\n",
      "[70] loss: 11.57626 | accuracy: 86.73%\n",
      "[71] loss: 11.45352 | accuracy: 86.69%\n",
      "[72] loss: 11.65838 | accuracy: 86.62%\n",
      "[73] loss: 11.55407 | accuracy: 86.64%\n",
      "[74] loss: 11.39883 | accuracy: 86.28%\n",
      "[75] loss: 11.53215 | accuracy: 86.83%\n",
      "[76] loss: 11.56392 | accuracy: 86.54%\n",
      "[77] loss: 11.34449 | accuracy: 87.09%\n",
      "[78] loss: 11.36107 | accuracy: 86.72%\n",
      "[79] loss: 11.41586 | accuracy: 86.51%\n",
      "[80] loss: 11.45906 | accuracy: 86.42%\n",
      "[81] loss: 11.28249 | accuracy: 86.82%\n",
      "[82] loss: 11.49026 | accuracy: 86.50%\n",
      "[83] loss: 11.37844 | accuracy: 86.86%\n",
      "[84] loss: 11.23432 | accuracy: 86.26%\n",
      "[85] loss: 11.18740 | accuracy: 86.26%\n",
      "[86] loss: 11.23647 | accuracy: 86.41%\n",
      "[87] loss: 10.99523 | accuracy: 86.51%\n",
      "[88] loss: 11.33686 | accuracy: 86.83%\n",
      "[89] loss: 11.16422 | accuracy: 86.74%\n",
      "[90] loss: 11.07648 | accuracy: 86.90%\n",
      "[91] loss: 10.91876 | accuracy: 86.74%\n",
      "[92] loss: 10.98303 | accuracy: 86.56%\n",
      "[93] loss: 11.12327 | accuracy: 86.93%\n",
      "[94] loss: 11.07850 | accuracy: 86.90%\n",
      "[95] loss: 11.06595 | accuracy: 86.98%\n",
      "[96] loss: 10.94124 | accuracy: 86.63%\n",
      "[97] loss: 10.79852 | accuracy: 86.50%\n",
      "[98] loss: 10.91412 | accuracy: 87.01%\n",
      "[99] loss: 10.93815 | accuracy: 87.13%\n",
      "[100] loss: 10.99369 | accuracy: 86.96%\n",
      "=== Finished distillation: 0.1 | 3 | adam ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.96 %\n",
      "\tBaseline accuracy on test: 84.39 %\n",
      "\tDiff: 3.0700000000000074 2.569999999999993\n",
      "\n",
      "=== DISTILLATION: 0.5 | 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "261231f51ab34277bfa4e168f70a2da5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 41.51855 | accuracy: 50.78%\n",
      "[2] loss: 30.89795 | accuracy: 66.88%\n",
      "[3] loss: 25.17502 | accuracy: 71.89%\n",
      "[4] loss: 21.76263 | accuracy: 77.06%\n",
      "[5] loss: 19.19343 | accuracy: 77.62%\n",
      "[6] loss: 17.57857 | accuracy: 80.05%\n",
      "[7] loss: 16.37826 | accuracy: 80.30%\n",
      "[8] loss: 15.46283 | accuracy: 81.75%\n",
      "[9] loss: 14.58701 | accuracy: 82.04%\n",
      "[10] loss: 14.03727 | accuracy: 82.98%\n",
      "[11] loss: 13.27051 | accuracy: 81.74%\n",
      "[12] loss: 12.74381 | accuracy: 83.55%\n",
      "[13] loss: 12.19071 | accuracy: 83.41%\n",
      "[14] loss: 11.92919 | accuracy: 83.67%\n",
      "[15] loss: 11.47560 | accuracy: 84.18%\n",
      "[16] loss: 11.16885 | accuracy: 84.07%\n",
      "[17] loss: 10.79427 | accuracy: 84.42%\n",
      "[18] loss: 10.49152 | accuracy: 84.20%\n",
      "[19] loss: 10.21669 | accuracy: 84.68%\n",
      "[20] loss: 9.98903 | accuracy: 84.58%\n",
      "[21] loss: 9.80352 | accuracy: 84.73%\n",
      "[22] loss: 9.57314 | accuracy: 84.64%\n",
      "[23] loss: 9.31354 | accuracy: 85.00%\n",
      "[24] loss: 9.26908 | accuracy: 84.22%\n",
      "[25] loss: 9.04260 | accuracy: 85.43%\n",
      "[26] loss: 9.09656 | accuracy: 85.05%\n",
      "[27] loss: 8.91184 | accuracy: 84.84%\n",
      "[28] loss: 8.60801 | accuracy: 85.37%\n",
      "[29] loss: 8.59363 | accuracy: 84.68%\n",
      "[30] loss: 8.35825 | accuracy: 85.14%\n",
      "[31] loss: 8.34429 | accuracy: 85.74%\n",
      "[32] loss: 8.21984 | accuracy: 85.62%\n",
      "[33] loss: 8.19101 | accuracy: 85.23%\n",
      "[34] loss: 8.14612 | accuracy: 85.55%\n",
      "[35] loss: 8.05654 | accuracy: 85.81%\n",
      "[36] loss: 7.93459 | accuracy: 85.22%\n",
      "[37] loss: 7.79292 | accuracy: 85.64%\n",
      "[38] loss: 7.67441 | accuracy: 85.29%\n",
      "[39] loss: 7.68321 | accuracy: 85.70%\n",
      "[40] loss: 7.59953 | accuracy: 85.13%\n",
      "[41] loss: 7.61421 | accuracy: 84.49%\n",
      "[42] loss: 7.61841 | accuracy: 85.29%\n",
      "[43] loss: 7.44217 | accuracy: 85.47%\n",
      "[44] loss: 7.44759 | accuracy: 85.43%\n",
      "[45] loss: 7.40118 | accuracy: 85.64%\n",
      "[46] loss: 7.30032 | accuracy: 85.74%\n",
      "[47] loss: 7.28345 | accuracy: 85.57%\n",
      "[48] loss: 7.33110 | accuracy: 85.81%\n",
      "[49] loss: 7.11505 | accuracy: 85.74%\n",
      "[50] loss: 7.13237 | accuracy: 85.46%\n",
      "[51] loss: 7.10368 | accuracy: 85.77%\n",
      "[52] loss: 7.16941 | accuracy: 85.76%\n",
      "[53] loss: 7.10285 | accuracy: 85.59%\n",
      "[54] loss: 7.10883 | accuracy: 85.52%\n",
      "[55] loss: 6.99932 | accuracy: 85.31%\n",
      "[56] loss: 6.90257 | accuracy: 85.38%\n",
      "[57] loss: 6.97461 | accuracy: 85.24%\n",
      "[58] loss: 6.90474 | accuracy: 85.73%\n",
      "[59] loss: 6.82666 | accuracy: 86.05%\n",
      "[60] loss: 6.85557 | accuracy: 86.15%\n",
      "[61] loss: 6.83858 | accuracy: 85.77%\n",
      "[62] loss: 6.80407 | accuracy: 85.70%\n",
      "[63] loss: 6.76170 | accuracy: 85.72%\n",
      "[64] loss: 6.80770 | accuracy: 85.92%\n",
      "[65] loss: 6.73271 | accuracy: 85.94%\n",
      "[66] loss: 6.74921 | accuracy: 86.20%\n",
      "[67] loss: 6.68271 | accuracy: 86.08%\n",
      "[68] loss: 6.59945 | accuracy: 86.01%\n",
      "[69] loss: 6.64777 | accuracy: 86.07%\n",
      "[70] loss: 6.72481 | accuracy: 86.29%\n",
      "[71] loss: 6.62421 | accuracy: 85.50%\n",
      "[72] loss: 6.59597 | accuracy: 85.92%\n",
      "[73] loss: 6.69305 | accuracy: 86.57%\n",
      "[74] loss: 6.49766 | accuracy: 85.46%\n",
      "[75] loss: 6.61294 | accuracy: 86.15%\n",
      "[76] loss: 6.44129 | accuracy: 85.72%\n",
      "[77] loss: 6.49873 | accuracy: 85.86%\n",
      "[78] loss: 6.51463 | accuracy: 86.02%\n",
      "[79] loss: 6.47169 | accuracy: 85.85%\n",
      "[80] loss: 6.37546 | accuracy: 86.29%\n",
      "[81] loss: 6.45145 | accuracy: 85.57%\n",
      "[82] loss: 6.36975 | accuracy: 85.71%\n",
      "[83] loss: 6.34022 | accuracy: 86.12%\n",
      "[84] loss: 6.34587 | accuracy: 86.32%\n",
      "[85] loss: 6.37252 | accuracy: 86.15%\n",
      "[86] loss: 6.28935 | accuracy: 86.02%\n",
      "[87] loss: 6.25609 | accuracy: 86.44%\n",
      "[88] loss: 6.24791 | accuracy: 85.79%\n",
      "[89] loss: 6.25768 | accuracy: 86.10%\n",
      "[90] loss: 6.16755 | accuracy: 86.14%\n",
      "[91] loss: 6.26841 | accuracy: 86.27%\n",
      "[92] loss: 6.27721 | accuracy: 86.25%\n",
      "[93] loss: 6.27166 | accuracy: 86.36%\n",
      "[94] loss: 6.23122 | accuracy: 86.35%\n",
      "[95] loss: 6.19315 | accuracy: 86.53%\n",
      "[96] loss: 6.22053 | accuracy: 86.26%\n",
      "[97] loss: 6.22076 | accuracy: 85.86%\n",
      "[98] loss: 6.17628 | accuracy: 86.54%\n",
      "[99] loss: 6.18123 | accuracy: 86.14%\n",
      "[100] loss: 6.12883 | accuracy: 86.25%\n",
      "=== Finished distillation: 0.5 | 3 | adam ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.25 %\n",
      "\tBaseline accuracy on test: 84.39 %\n",
      "\tDiff: 3.780000000000001 1.8599999999999994\n",
      "\n",
      "=== DISTILLATION: 0.7 | 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed219f397dd14cd6b707739dc537576b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 27.20183 | accuracy: 51.34%\n",
      "[2] loss: 20.42100 | accuracy: 64.93%\n",
      "[3] loss: 16.84263 | accuracy: 71.50%\n",
      "[4] loss: 14.32314 | accuracy: 76.79%\n",
      "[5] loss: 12.77414 | accuracy: 78.29%\n",
      "[6] loss: 11.66415 | accuracy: 78.76%\n",
      "[7] loss: 10.77147 | accuracy: 81.14%\n",
      "[8] loss: 10.13765 | accuracy: 81.24%\n",
      "[9] loss: 9.58090 | accuracy: 82.77%\n",
      "[10] loss: 9.11221 | accuracy: 82.41%\n",
      "[11] loss: 8.67924 | accuracy: 83.18%\n",
      "[12] loss: 8.31265 | accuracy: 83.52%\n",
      "[13] loss: 8.03719 | accuracy: 83.41%\n",
      "[14] loss: 7.76163 | accuracy: 84.11%\n",
      "[15] loss: 7.47510 | accuracy: 83.84%\n",
      "[16] loss: 7.23804 | accuracy: 83.61%\n",
      "[17] loss: 6.92237 | accuracy: 85.05%\n",
      "[18] loss: 6.81590 | accuracy: 84.47%\n",
      "[19] loss: 6.69051 | accuracy: 85.16%\n",
      "[20] loss: 6.46703 | accuracy: 84.46%\n",
      "[21] loss: 6.31510 | accuracy: 85.44%\n",
      "[22] loss: 6.09472 | accuracy: 84.89%\n",
      "[23] loss: 6.04276 | accuracy: 85.23%\n",
      "[24] loss: 5.94728 | accuracy: 85.18%\n",
      "[25] loss: 5.80775 | accuracy: 85.00%\n",
      "[26] loss: 5.65579 | accuracy: 84.87%\n",
      "[27] loss: 5.59470 | accuracy: 85.43%\n",
      "[28] loss: 5.49539 | accuracy: 85.45%\n",
      "[29] loss: 5.42427 | accuracy: 86.02%\n",
      "[30] loss: 5.36518 | accuracy: 85.43%\n",
      "[31] loss: 5.31715 | accuracy: 85.48%\n",
      "[32] loss: 5.15130 | accuracy: 85.30%\n",
      "[33] loss: 5.14430 | accuracy: 85.56%\n",
      "[34] loss: 5.02168 | accuracy: 85.97%\n",
      "[35] loss: 5.10357 | accuracy: 85.32%\n",
      "[36] loss: 4.97577 | accuracy: 85.60%\n",
      "[37] loss: 4.80385 | accuracy: 85.82%\n",
      "[38] loss: 4.81666 | accuracy: 85.76%\n",
      "[39] loss: 4.80509 | accuracy: 85.65%\n",
      "[40] loss: 4.69326 | accuracy: 85.82%\n",
      "[41] loss: 4.73393 | accuracy: 85.53%\n",
      "[42] loss: 4.71717 | accuracy: 85.99%\n",
      "[43] loss: 4.67233 | accuracy: 85.80%\n",
      "[44] loss: 4.65970 | accuracy: 85.82%\n",
      "[45] loss: 4.66091 | accuracy: 85.73%\n",
      "[46] loss: 4.64688 | accuracy: 85.52%\n",
      "[47] loss: 4.56044 | accuracy: 85.71%\n",
      "[48] loss: 4.53403 | accuracy: 85.96%\n",
      "[49] loss: 4.59381 | accuracy: 85.62%\n",
      "[50] loss: 4.50050 | accuracy: 86.22%\n",
      "[51] loss: 4.39784 | accuracy: 85.71%\n",
      "[52] loss: 4.46272 | accuracy: 86.16%\n",
      "[53] loss: 4.36901 | accuracy: 86.00%\n",
      "[54] loss: 4.34720 | accuracy: 86.16%\n",
      "[55] loss: 4.40764 | accuracy: 85.88%\n",
      "[56] loss: 4.30921 | accuracy: 85.99%\n",
      "[57] loss: 4.28437 | accuracy: 85.85%\n",
      "[58] loss: 4.34548 | accuracy: 86.12%\n",
      "[59] loss: 4.26016 | accuracy: 85.84%\n",
      "[60] loss: 4.23785 | accuracy: 86.31%\n",
      "[61] loss: 4.26234 | accuracy: 86.22%\n",
      "[62] loss: 4.28352 | accuracy: 85.93%\n",
      "[63] loss: 4.21644 | accuracy: 86.10%\n",
      "[64] loss: 4.17886 | accuracy: 85.94%\n",
      "[65] loss: 4.15542 | accuracy: 86.19%\n",
      "[66] loss: 4.14798 | accuracy: 86.21%\n",
      "[67] loss: 4.14675 | accuracy: 86.47%\n",
      "[68] loss: 4.13204 | accuracy: 85.86%\n",
      "[69] loss: 4.08543 | accuracy: 86.20%\n",
      "[70] loss: 4.11659 | accuracy: 86.45%\n",
      "[71] loss: 4.10751 | accuracy: 85.83%\n",
      "[72] loss: 4.06846 | accuracy: 86.14%\n",
      "[73] loss: 4.08072 | accuracy: 86.17%\n",
      "[74] loss: 4.10175 | accuracy: 86.15%\n",
      "[75] loss: 4.03041 | accuracy: 86.64%\n",
      "[76] loss: 4.08588 | accuracy: 86.08%\n",
      "[77] loss: 3.99219 | accuracy: 86.01%\n",
      "[78] loss: 4.04320 | accuracy: 86.24%\n",
      "[79] loss: 3.97100 | accuracy: 85.81%\n",
      "[80] loss: 4.04187 | accuracy: 86.11%\n",
      "[81] loss: 3.94321 | accuracy: 86.06%\n",
      "[82] loss: 3.95847 | accuracy: 86.27%\n",
      "[83] loss: 3.92675 | accuracy: 86.03%\n",
      "[84] loss: 3.91637 | accuracy: 85.75%\n",
      "[85] loss: 3.90507 | accuracy: 86.26%\n",
      "[86] loss: 3.90673 | accuracy: 85.91%\n",
      "[87] loss: 3.93388 | accuracy: 86.37%\n",
      "[88] loss: 3.88074 | accuracy: 86.41%\n",
      "[89] loss: 3.87065 | accuracy: 85.98%\n",
      "[90] loss: 3.85500 | accuracy: 86.31%\n",
      "[91] loss: 3.84299 | accuracy: 86.22%\n",
      "[92] loss: 3.85953 | accuracy: 86.48%\n",
      "[93] loss: 3.87789 | accuracy: 86.52%\n",
      "[94] loss: 3.81717 | accuracy: 86.28%\n",
      "[95] loss: 3.82670 | accuracy: 86.10%\n",
      "[96] loss: 3.78050 | accuracy: 86.52%\n",
      "[97] loss: 3.77817 | accuracy: 86.33%\n",
      "[98] loss: 3.88047 | accuracy: 86.32%\n",
      "[99] loss: 3.78564 | accuracy: 86.70%\n",
      "[100] loss: 3.80202 | accuracy: 86.30%\n",
      "=== Finished distillation: 0.7 | 3 | adam ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.3 %\n",
      "\tBaseline accuracy on test: 84.39 %\n",
      "\tDiff: 3.730000000000004 1.9099999999999966\n",
      "\n",
      "=== DISTILLATION: 0.9 | 3 | adam ===\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86b46f4214704a77b7deeece64cb86e8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 10.82560 | accuracy: 48.30%\n",
      "[2] loss: 8.80848 | accuracy: 62.97%\n",
      "[3] loss: 7.47887 | accuracy: 65.75%\n",
      "[4] loss: 6.45710 | accuracy: 73.14%\n",
      "[5] loss: 5.77539 | accuracy: 75.49%\n",
      "[6] loss: 5.32330 | accuracy: 77.20%\n",
      "[7] loss: 4.87714 | accuracy: 79.30%\n",
      "[8] loss: 4.54973 | accuracy: 79.61%\n",
      "[9] loss: 4.35258 | accuracy: 78.60%\n",
      "[10] loss: 4.12981 | accuracy: 80.99%\n",
      "[11] loss: 3.95223 | accuracy: 81.21%\n",
      "[12] loss: 3.76738 | accuracy: 82.54%\n",
      "[13] loss: 3.59874 | accuracy: 82.84%\n",
      "[14] loss: 3.49506 | accuracy: 82.03%\n",
      "[15] loss: 3.37713 | accuracy: 83.94%\n",
      "[16] loss: 3.26906 | accuracy: 82.82%\n",
      "[17] loss: 3.16229 | accuracy: 83.68%\n",
      "[18] loss: 3.05739 | accuracy: 84.14%\n",
      "[19] loss: 2.98605 | accuracy: 83.97%\n",
      "[20] loss: 2.87266 | accuracy: 84.30%\n",
      "[21] loss: 2.79890 | accuracy: 84.07%\n",
      "[22] loss: 2.77672 | accuracy: 84.13%\n",
      "[23] loss: 2.69499 | accuracy: 85.13%\n",
      "[24] loss: 2.60966 | accuracy: 84.21%\n",
      "[25] loss: 2.55508 | accuracy: 84.30%\n",
      "[26] loss: 2.52615 | accuracy: 84.72%\n",
      "[27] loss: 2.43657 | accuracy: 84.09%\n",
      "[28] loss: 2.41723 | accuracy: 85.32%\n",
      "[29] loss: 2.38515 | accuracy: 85.19%\n",
      "[30] loss: 2.31971 | accuracy: 85.56%\n",
      "[31] loss: 2.28772 | accuracy: 85.27%\n",
      "[32] loss: 2.24883 | accuracy: 84.94%\n",
      "[33] loss: 2.21786 | accuracy: 85.02%\n",
      "[34] loss: 2.18188 | accuracy: 85.64%\n",
      "[35] loss: 2.14738 | accuracy: 85.46%\n",
      "[36] loss: 2.10744 | accuracy: 85.30%\n",
      "[37] loss: 2.08591 | accuracy: 85.63%\n",
      "[38] loss: 2.06671 | accuracy: 85.50%\n",
      "[39] loss: 2.03399 | accuracy: 84.97%\n",
      "[40] loss: 1.99337 | accuracy: 85.38%\n",
      "[41] loss: 1.95843 | accuracy: 85.81%\n",
      "[42] loss: 1.93441 | accuracy: 85.87%\n",
      "[43] loss: 1.97649 | accuracy: 85.89%\n",
      "[44] loss: 1.94332 | accuracy: 85.69%\n",
      "[45] loss: 1.92395 | accuracy: 85.54%\n",
      "[46] loss: 1.87812 | accuracy: 85.77%\n",
      "[47] loss: 1.86793 | accuracy: 85.20%\n",
      "[48] loss: 1.83506 | accuracy: 85.57%\n",
      "[49] loss: 1.82615 | accuracy: 85.85%\n",
      "[50] loss: 1.83260 | accuracy: 86.26%\n",
      "[51] loss: 1.82078 | accuracy: 85.64%\n",
      "[52] loss: 1.81140 | accuracy: 85.96%\n",
      "[53] loss: 1.79783 | accuracy: 85.58%\n",
      "[54] loss: 1.73957 | accuracy: 85.73%\n",
      "[55] loss: 1.74792 | accuracy: 85.54%\n",
      "[56] loss: 1.70398 | accuracy: 85.81%\n",
      "[57] loss: 1.74442 | accuracy: 86.02%\n",
      "[58] loss: 1.75784 | accuracy: 85.48%\n",
      "[59] loss: 1.79230 | accuracy: 85.63%\n",
      "[60] loss: 1.69914 | accuracy: 85.82%\n",
      "[61] loss: 1.67702 | accuracy: 86.47%\n",
      "[62] loss: 1.67786 | accuracy: 85.77%\n",
      "[63] loss: 1.69400 | accuracy: 86.05%\n",
      "[64] loss: 1.65027 | accuracy: 85.16%\n",
      "[65] loss: 1.65242 | accuracy: 85.92%\n",
      "[66] loss: 1.63833 | accuracy: 85.73%\n",
      "[67] loss: 1.63181 | accuracy: 85.76%\n",
      "[68] loss: 1.63343 | accuracy: 85.76%\n",
      "[69] loss: 1.60071 | accuracy: 85.75%\n",
      "[70] loss: 1.59453 | accuracy: 85.72%\n",
      "[71] loss: 1.60288 | accuracy: 86.17%\n",
      "[72] loss: 1.57523 | accuracy: 85.63%\n",
      "[73] loss: 1.59166 | accuracy: 85.82%\n",
      "[74] loss: 1.58009 | accuracy: 85.91%\n",
      "[75] loss: 1.56656 | accuracy: 85.71%\n",
      "[76] loss: 1.58054 | accuracy: 86.30%\n",
      "[77] loss: 1.57275 | accuracy: 85.94%\n",
      "[78] loss: 1.59427 | accuracy: 86.35%\n",
      "[79] loss: 1.54908 | accuracy: 85.98%\n",
      "[80] loss: 1.55713 | accuracy: 85.89%\n",
      "[81] loss: 1.54897 | accuracy: 86.00%\n",
      "[82] loss: 1.54571 | accuracy: 85.77%\n",
      "[83] loss: 1.51678 | accuracy: 85.66%\n",
      "[84] loss: 1.54099 | accuracy: 85.80%\n",
      "[85] loss: 1.54485 | accuracy: 85.84%\n",
      "[86] loss: 1.53088 | accuracy: 86.13%\n",
      "[87] loss: 1.50919 | accuracy: 85.81%\n",
      "[88] loss: 1.47729 | accuracy: 86.13%\n",
      "[89] loss: 1.51248 | accuracy: 86.14%\n",
      "[90] loss: 1.47077 | accuracy: 85.75%\n",
      "[91] loss: 1.49011 | accuracy: 85.70%\n",
      "[92] loss: 1.47614 | accuracy: 86.07%\n",
      "[93] loss: 1.48048 | accuracy: 86.15%\n",
      "[94] loss: 1.46273 | accuracy: 86.37%\n",
      "[95] loss: 1.48794 | accuracy: 86.04%\n",
      "[96] loss: 1.45600 | accuracy: 85.83%\n",
      "[97] loss: 1.45549 | accuracy: 86.51%\n",
      "[98] loss: 1.43536 | accuracy: 86.11%\n",
      "[99] loss: 1.45662 | accuracy: 85.91%\n",
      "[100] loss: 1.46541 | accuracy: 86.07%\n",
      "=== Finished distillation: 0.9 | 3 | adam ===\n",
      "\tTotal number of teacher params: 134309962\n",
      "\tTotal number of learner params: 4917902\n",
      "\tTotal reduction: 96.33839372242544 %\n",
      "\tTeacher  accuracy on test: 90.03 %\n",
      "\tLearner  accuracy on test: 86.07 %\n",
      "\tBaseline accuracy on test: 84.39 %\n",
      "\tDiff: 3.960000000000008 1.6799999999999926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kind in (3,):\n",
    "    for opt in (\"sgd\", \"adam\"):\n",
    "        BASELINE_ACC = train_baseline(kind=kind, opt=opt, coef=1)\n",
    "        for a in (0, 0.1, 0.5, 0.7, 0.9):\n",
    "            distil(a=a, kind=kind, opt=opt, coef=1)"
   ]
  },
  {
   "source": [
    "We can clearly see that, in general, the distilled network performed on test better than the baseline, i.e., the same net trained on the dataset.\n",
    "\n",
    "The final accuracy of the teacher network (VGG16 with batchnormalization) is about 90%\n",
    "\n",
    "The accuracy of the baseline is about 86.16% while the accuracy of the corresponding distilled network is about 87.61%.\n",
    "\n",
    "To summarize, the total number of parameters is reduced by 96% while decrease in accuracy in coparison with teacher is about 2.4%. This gives improvement of the overall performance in comparison with the baseline at the level of 1.5%. Unfortunately, the decrease in the accuracy vs. teacher is greater than improvement vs. baseline.\n",
    "\n",
    "To conclude, this results, in my opinion, are acceptable as noticeable improvement with significant reduction of the teacher network was achieved."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For example,\n",
    "\n",
    "```=== Finished distillation: 0.9 | 3 | sgd ===\n",
    "\tTotal number of teacher params: 134309962\n",
    "\tTotal number of learner params: 4917902\n",
    "\tTotal reduction: 96.33839372242544 %\n",
    "\tTeacher  accuracy on test: 90.03 %\n",
    "\tLearner  accuracy on test: 87.61 %\n",
    "\tBaseline accuracy on test: 86.16 %\n",
    "\tDiff: 2.4200000000000017 1.4500000000000028\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}