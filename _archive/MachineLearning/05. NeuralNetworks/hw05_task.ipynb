{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw05_task.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5-final"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9Qp0H_zUQuu_"},"source":["# Нейронные сети\n","__Суммарное количество баллов: 10__\n","\n","__Решение отправлять на `ml.course.practice@gmail.com`__\n","\n","__Тема письма: `[ML][HW05] <ФИ>`, где вместо `<ФИ>` указаны фамилия и имя__\n","\n","Для начала вам предстоит реализовать свой собственный backpropagation и протестировать его на реальных данных, а затем научиться обучать нейронные сети при помощи библиотеки `PyTorch` и использовать это умение для классификации классического набора данных CIFAR10."]},{"cell_type":"code","metadata":{"id":"22ezVRf3QuvA"},"source":["import numpy as np\n","import copy\n","from sklearn.datasets import make_blobs, make_moons\n","from typing import List, NoReturn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4qfDPH_LQuvF"},"source":["### Задание 1 (3 балла)\n","Нейронные сети состоят из слоев, поэтому для начала понадобится реализовать их. Пока нам понадобятся только три:\n","\n","`Linear` - полносвязный слой, в котором `y = Wx + b`, где `y` - выход, `x` - вход, `W` - матрица весов, а `b` - смещение. \n","\n","`ReLU` - слой, соответствующий функции активации `y = max(0, x)`.\n","\n","`Softmax` - слой, соответствующий функции активации [softmax](https://ru.wikipedia.org/wiki/Softmax)\n","\n","\n","#### Методы\n","`forward(X)` - возвращает предсказанные для `X`. `X` может быть как вектором, так и батчем\n","\n","`backward(d)` - считает градиент при помощи обратного распространения ошибки. Возвращает новое значение `d`\n","\n","`update(alpha)` - обновляет веса (если необходимо) с заданой скоростью обучения"]},{"cell_type":"code","metadata":{"id":"RWFLlHqaYbgC"},"source":["class Module:\n","    \"\"\"\n","    Абстрактный класс. Его менять не нужно.\n","    \"\"\"\n","    def forward(self, x):\n","        raise NotImplementedError()\n","    \n","    def backward(self, d):\n","        raise NotImplementedError()\n","        \n","    def update(self, alpha):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYS2gE4PYepZ"},"source":["class Linear(Module):\n","    \"\"\"\n","    Линейный полносвязный слой.\n","    \"\"\"\n","    def __init__(self, in_features: int, out_features: int):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        in_features : int\n","            Размер входа.\n","        out_features : int \n","            Размер выхода.\n","    \n","        Notes\n","        -----\n","        W и b инициализируются случайно.\n","        \"\"\"\n","        boundary = 1 / np.sqrt(in_features)\n","        self.W = np.random.uniform(-boundary, boundary, size=(in_features, out_features))\n","        # self.W = np.random.uniform(size=(in_features, out_features))\n","        self.b = np.zeros(shape=(1, out_features))\n","\n","    \n","    def forward(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Возвращает y = XW + b.\n","\n","        Parameters\n","        ----------\n","        x : np.ndarray\n","            Входной вектор или батч.\n","            То есть, либо x вектор с in_features элементов,\n","            либо матрица размерности (batch_size, in_features).\n","    \n","        Return\n","        ------\n","        y : np.ndarray\n","            Выход после слоя.\n","            Либо вектор с out_features элементами,\n","            либо матрица размерности (batch_size, out_features)\n","\n","        \"\"\"\n","        self.X = X\n","        return self.X @ self.W + self.b\n","    \n","    def backward(self, d: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Cчитает градиент при помощи обратного распространения ошибки.\n","\n","        Parameters\n","        ----------\n","        d : np.ndarray\n","            Градиент.\n","        Return\n","        ------\n","        np.ndarray\n","            Новое значение градиента.\n","        \"\"\"\n","        self.dW = self.X.T @ d\n","        self.db = np.sum(d, axis=0, keepdims=True)\n","        return d @ self.W.T\n","        \n","    def update(self, alpha: float) -> NoReturn:\n","        \"\"\"\n","        Обновляет W и b с заданной скоростью обучения.\n","\n","        Parameters\n","        ----------\n","        alpha : float\n","            Скорость обучения.\n","        \"\"\"\n","        self.W -= alpha * self.dW  # GD\n","        self.b -= alpha * self.db"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def one_hot(Y):\n","    Y = np.array(Y)\n","    Y_ = np.zeros((Y.size, Y.max() + 1))\n","    Y_[np.arange(Y.size), Y] = 1\n","    return Y_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def softmax(x):\n","    exp_ = np.exp(x)\n","    return exp_ / np.sum(exp_, axis=1, keepdims=True)"]},{"cell_type":"code","metadata":{"id":"94hkbnD1QuvG"},"source":["class ReLU(Module):\n","    \"\"\"\n","    Слой, соответствующий функции активации ReLU.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Возвращает y = max(0, x).\n","\n","        Parameters\n","        ----------\n","        x : np.ndarray\n","            Входной вектор или батч.\n","    \n","        Return\n","        ------\n","        y : np.ndarray\n","            Выход после слоя (той же размерности, что и вход).\n","\n","        \"\"\"\n","        self.X = X\n","        return np.maximum(X, 0)\n","        \n","    def backward(self, d) -> np.ndarray:\n","        \"\"\"\n","        Cчитает градиент при помощи обратного распространения ошибки.\n","\n","        Parameters\n","        ----------\n","        d : np.ndarray\n","            Градиент.\n","            \n","        Return\n","        ------\n","        np.ndarray\n","            Новое значение градиента.\n","        \"\"\"\n","        return d * (self.X >= 0)\n","        \n","        \n","class CrossEntropyLoss(Module):\n","    \"\"\"\n","    Слой, соответствующий функции активации Softmax.\n","    \"\"\"\n","    def __init__(self):\n","        pass\n","    \n","    def forward(self, X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Возвращает CrossEntropy(Softmax(x), y).\n","\n","        Parameters\n","        ----------\n","        x : np.ndarray\n","            Входной вектор или батч.\n","    \n","        Return\n","        ------\n","        Loss : np.ndarray\n","            Выход после слоя (той же размерности, что и вход).\n","        \"\"\"\n","        self.sigma = softmax(X)\n","        self.Y = Y\n","        return -np.sum(Y * np.log(self.sigma)) / X.shape[0]\n","        \n","    def backward(self, d) -> np.ndarray:\n","        \"\"\"\n","        Cчитает градиент при помощи обратного распространения ошибки.\n","\n","        Parameters\n","        ----------\n","        d : np.ndarray\n","            Градиент.\n","\n","        Return\n","        ------\n","        np.ndarray\n","            Новое значение градиента.\n","        \"\"\"\n","        return (self.sigma - self.Y) * d"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rb_ip_h8QuvJ"},"source":["### Задание 2 (2 балла)\n","Теперь сделаем саму нейронную сеть.\n","\n","#### Методы\n","`fit(X, y)` - обучает нейронную сеть заданное число эпох. В каждой эпохе необходимо использовать [cross-entropy loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy) для обучения, а так же производить обновления не по одному элементу, а используя батчи.\n","\n","`predict_proba(X)` - предсказывает вероятности классов для элементов `X`\n","\n","#### Параметры конструктора\n","`modules` - список, состоящий из ранее реализованных модулей и описывающий слои нейронной сети. В конец необходимо добавить `Softmax`\n","\n","`epochs` - количество эпох обучения\n","\n","`alpha` - скорость обучения"]},{"cell_type":"code","metadata":{"id":"Q_JFCizKQuvK"},"source":["class MLPClassifier:\n","    def __init__(self, modules: List[Module], epochs: int = 40, alpha: float = 0.01):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        modules : List[Module]\n","            Cписок, состоящий из ранее реализованных модулей и \n","            описывающий слои нейронной сети. \n","            В конец необходимо добавить Softmax.\n","        epochs : int\n","            Количество эпох обученияю\n","        alpha : float\n","            Cкорость обучения.\n","        \"\"\"\n","        self.modules = modules\n","        self.epochs = epochs\n","        self.alpha = alpha\n","        self.loss = CrossEntropyLoss()\n","            \n","    def fit(self, X: np.ndarray, y: np.ndarray, batch_size=32) -> NoReturn:\n","        \"\"\"\n","        Обучает нейронную сеть заданное число эпох. \n","        В каждой эпохе необходимо использовать cross-entropy loss для обучения, \n","        а так же производить обновления не по одному элементу, а используя батчи.\n","\n","        Parameters\n","        ----------\n","        X : np.ndarray\n","            Данные для обучения.\n","        y : np.ndarray\n","            Вектор меток классов для данных.\n","        batch_size : int\n","            Размер батча.\n","        \"\"\"\n","        Y = one_hot(y)\n","        idxs = np.arange(X.shape[0])\n","        for _ in range(self.epochs):\n","            np.random.shuffle(idxs)\n","            batches = np.array_split(idxs, idxs.shape[0] / batch_size)\n","            for batch in batches:\n","                Y_ = Y[batch].copy()\n","                X_ = X[batch].copy()\n","                for module in self.modules:  # forward\n","                    X_ = module.forward(X_)\n","                loss = self.loss.forward(X_, Y_)\n","                d = self.loss.backward(1)  # backward\n","                for module in reversed(self.modules):\n","                    d = module.backward(d)\n","\n","                for module in self.modules:\n","                    module.update(self.alpha)\n","        \n","    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        Предсказывает вероятности классов для элементов X.\n","\n","        Parameters\n","        ----------\n","        X : np.ndarray\n","            Данные для предсказания.\n","        \n","        Return\n","        ------\n","        np.ndarray\n","            Предсказанные вероятности классов для всех элементов X.\n","            Размерность (X.shape[0], n_classes)\n","        \"\"\"\n","        X_ = X.copy()\n","        for module in self.modules:  # forward\n","            X_ = module.forward(X_)\n","        return softmax(X_)\n","        \n","    def predict(self, X) -> np.ndarray:\n","        \"\"\"\n","        Предсказывает метки классов для элементов X.\n","\n","        Parameters\n","        ----------\n","        X : np.ndarray\n","            Данные для предсказания.\n","        \n","        Return\n","        ------\n","        np.ndarray\n","            Вектор предсказанных классов\n","        \n","        \"\"\"\n","        p = self.predict_proba(X)\n","        return np.argmax(p, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"onDymYQXQuvN"},"source":["p = MLPClassifier([\n","    Linear(4, 64),\n","    ReLU(),\n","    Linear(64, 64),\n","    ReLU(),\n","    Linear(64, 2)\n","])\n","\n","X = np.random.randn(50, 4)\n","y = [(0 if x[0] > x[2]**2 or x[3]**3 > 0.5 else 1) for x in X]\n","p.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3C1EIsDqQuvQ"},"source":["### Задание 3 (2 балла)\n","Протестируем наше решение на синтетических данных. Необходимо подобрать гиперпараметры, при которых качество полученных классификаторов будет достаточным.\n","\n","#### Оценка\n","Accuracy на первом датасете больше 0.85 - +1 балл\n","\n","Accuracy на втором датасете больше 0.85 - +1 балл"]},{"cell_type":"code","metadata":{"id":"d5UAgXTcQuvQ"},"source":["X, y = make_moons(400, noise=0.075)\n","X_test, y_test = make_moons(400, noise=0.075)\n","\n","best_acc = 0\n","for _ in range(25):\n","    p = MLPClassifier([\n","            Linear(2, 64),\n","            ReLU(),\n","            Linear(64, 64),\n","            ReLU(),\n","            Linear(64, 2)\n","        ])\n","\n","    p.fit(X, y)\n","    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n","print(\"Accuracy\", best_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMDJM4qFQuvT"},"source":["X, y = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n","X_test, y_test = make_blobs(400, 2, centers=[[0, 0], [2.5, 2.5], [-2.5, 3]])\n","best_acc = 0\n","for _ in range(25):\n","    p = MLPClassifier([\n","            Linear(2, 64),\n","            ReLU(),\n","            Linear(64, 64),\n","            ReLU(),\n","            Linear(64, 3)\n","        ])\n","\n","    p.fit(X, y)\n","    best_acc = max(np.mean(p.predict(X_test) == y_test), best_acc)\n","print(\"Accuracy\", best_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPbVTFnMQuvW"},"source":["## PyTorch\n","\n","Для выполнения следующего задания понадобится PyTorch. [Инструкция по установке](https://pytorch.org/get-started/locally/)\n","\n","Если у вас нет GPU, то можно использовать [Google Colab](https://colab.research.google.com/)"]},{"cell_type":"code","metadata":{"id":"tV0mJLu-QuvX"},"source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import torch\n","from tqdm import tqdm\n","from torch import nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUC_QqpAQuva"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","t = transforms.ToTensor()\n","\n","cifar_train = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=True, transform=t)\n","train_loader = DataLoader(cifar_train, batch_size=1024, shuffle=True, pin_memory=torch.cuda.is_available())\n","cifar_test = datasets.CIFAR10(\"datasets/cifar10\", download=True, train=False, transform=t)\n","test_loader = DataLoader(cifar_test, batch_size=1024, shuffle=False, pin_memory=torch.cuda.is_available())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rGmpjcFfQuvd"},"source":["### Задание 4 (3 балла)\n","А теперь поработам с настоящими нейронными сетями и настоящими данными. Необходимо реализовать сверточную нейронную сеть, которая будет классифицировать изображения из датасета CIFAR10. Имплементируйте класс `Model` и функцию `calculate_loss`. \n","\n","Обратите внимание, что `Model` должна считать в конце `softmax`, т.к. мы решаем задачу классификации. Соответствено, функция `calculate_loss` считает cross-entropy.\n","\n","Для успешного выполнения задания необходимо, чтобы `accuracy`, `mean precision` и `mean recall` были больше 0.5\n","\n","__Можно пользоваться всем содержимым библиотеки PyTorch.__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def conv_block(in_channels):\n","    return nn.Sequential(\n","            # ~0.74\n","            # nn.Conv2d(in_channels, 64, 3),\n","            # nn.ReLU(),\n","\n","            # nn.Conv2d(64, 128, 3),\n","            # nn.ReLU(),\n","            # nn.MaxPool2d(2, 2),\n","            \n","            # nn.Conv2d(128, 256, 3),\n","            # nn.ReLU(),\n","            # nn.MaxPool2d(2, 2),\n","\n","            # nn.Conv2d(256, 512, 3),\n","            # nn.ReLU(),\n","            # nn.MaxPool2d(2, 2)\n","\n","            # a la VGG\n","            nn.Conv2d(in_channels, 64, 3),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","\n","            nn.Conv2d(64, 128, 3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.Dropout(),\n","            nn.BatchNorm2d(128),\n","\n","            nn.Conv2d(128, 256, 3),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 256, 3),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2),\n","            nn.Dropout(),\n","            nn.BatchNorm2d(256),\n","\n","            nn.Conv2d(256, 512, 3),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(512),\n","            nn.Conv2d(512, 512, 3),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.BatchNorm2d(512),\n","            )"]},{"cell_type":"code","metadata":{"id":"5sRmTKwKQuve"},"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv = conv_block(3)\n","\n","        self.res0 = nn.Sequential(\n","            nn.Conv2d(3, 64, 3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","        self.res1 = nn.Sequential(\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","        )\n","        self.res2 = nn.Sequential(\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","        )\n","        self.res3 = nn.Sequential(\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, 3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","        )\n","        self.res4 = nn.Sequential(\n","            nn.Conv2d(64, 128, 3),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 128, 3),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","        )\n","        self.res5 = nn.Conv2d(128, 512, 3)\n","        self.res6 = nn.Sequential(\n","            nn.Conv2d(512, 512, 3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.Conv2d(512, 512, 3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","        )\n","\n","        self.fc = nn.Sequential(\n","            # ~74\n","            # nn.Linear(512 * 2 * 2, 1024),\n","            # nn.ReLU(),\n","            # nn.Linear(1024, 512),\n","            # nn.ReLU(),\n","            # nn.Linear(512, 10),\n","            # nn.Softmax(dim = 1)\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(256, 10),\n","            nn.Dropout(),\n","            nn.Softmax(dim = 1)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        out = self.conv(x)\n","\n","        # out = self.res0(x)\n","        # res = out\n","        \n","        # out = self.res1(out)\n","        # out += res\n","        # out = F.max_pool2d(out, 2, 2)\n","        # res = out\n","        \n","        # out = self.res2(out)\n","        # out += res\n","        # out = F.max_pool2d(out, 2, 2)\n","        # res = out\n","\n","        # out = self.res3(out)\n","        # out += res\n","\n","        # out = self.res4(out)\n","        # out = self.res5(out)\n","        # res = out\n","        # out = self.res6(out)\n","        # out += res\n","\n","        out = self.fc(out.flatten(start_dim=1))\n","        return out\n","        \n","def calculate_loss(X: torch.Tensor, y: torch.Tensor, model: Model):\n","    \"\"\"\n","    Cчитает cross-entropy.\n","\n","    Parameters\n","    ----------\n","    X : torch.Tensor\n","        Данные для обучения.\n","    y : torch.Tensor\n","        Метки классов.\n","    model : Model\n","        Модель, которую будем обучать.\n","\n","    \"\"\"\n","    y_ = F.one_hot(y)\n","    return -torch.sum(y_ * torch.log(model(X))) / X.shape[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAsLmkUqQuvh"},"source":["Теперь обучим нашу модель. Для этого используем ранее созданные batch loader'ы."]},{"cell_type":"code","metadata":{"id":"k5G8iMCeQuvh"},"source":["def train(model, epochs=100):\n","    min_loss = 10000\n","    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n","    train_losses = []\n","    test_losses = []\n","    for i in range(epochs):\n","        #Train\n","        loss_mean = 0\n","        elements = 0\n","        for X, y in iter(train_loader):\n","            X = X.to(device)\n","            y = y.to(device)\n","            loss = calculate_loss(X, y, model)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_mean += loss.item() * len(X)\n","            elements += len(X)\n","        train_losses.append(loss_mean / elements)\n","        #Test\n","        loss_mean = 0 \n","        elements = 0\n","        for X, y in iter(test_loader):\n","            X = X.to(device)\n","            y = y.to(device)\n","            loss = calculate_loss(X, y, model)\n","            loss_mean += loss.item() * len(X)\n","            elements += len(X)\n","        test_losses.append(loss_mean / elements)\n","        print(\"Epoch\", i, \"| Train loss\", train_losses[-1], \"| Test loss\", test_losses[-1])\n","        if test_losses[-1] <= min_loss:\n","            min_loss = test_losses[-1]\n","            torch.save(model, \"model_best\")\n","    torch.save(model, \"model_last\")\n","    return train_losses, test_losses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vmD9eWJOQuvl","scrolled":true},"source":["%%time\n","model = Model().to(device)\n","train_l, test_l = train(model, 100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OJNAuHjNQuvn"},"source":["Построим график функции потерь"]},{"cell_type":"code","metadata":{"id":"F6OEGqriQuvo"},"source":["plt.figure(figsize=(12, 6))\n","plt.plot(range(len(train_l)), train_l, label=\"train\")\n","plt.plot(range(len(test_l)), test_l, label=\"test\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"miUxg0bDQuvs"},"source":["И, наконец, посчитаем метрики"]},{"cell_type":"code","metadata":{"id":"UXSOJFI8Quvt"},"source":["if os.path.exists(\"model_best\"):\n","    model = torch.load(\"model_best\")\n","model.eval()\n","\n","true_positive = np.zeros(10)\n","true_negative = np.zeros(10)\n","false_positive = np.zeros(10)\n","false_negative = np.zeros(10)\n","accuracy = 0\n","ctn = 0\n","for X, y in iter(test_loader):\n","    X = X.to(device)\n","    y = y.to(device)\n","    with torch.no_grad():\n","        y_pred = model(X).max(dim=1)[1]\n","    for i in range(10):\n","        for pred, real in zip(y_pred, y):\n","            if real == i:\n","                if pred == real:\n","                    true_positive[i] += 1\n","                else:\n","                    false_negative[i] += 1\n","            else:\n","                if pred == i:\n","                    false_positive[i] += 1\n","                else:\n","                    true_negative[i] += 1\n","            \n","    accuracy += torch.sum(y_pred == y).item()\n","    ctn += len(y)\n","print(\"Overall accuracy\", accuracy / ctn)\n","print(\"Precision\", true_positive / (true_positive + false_positive))\n","print(\"Recall\", true_positive / (true_positive + false_negative))\n","print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n","print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKA-j4rIQuvv"},"source":["if exists(\"model_last\"):\n","    model = torch.load(\"model_last\")\n","model.eval()\n","\n","true_positive = np.zeros(10)\n","true_negative = np.zeros(10)\n","false_positive = np.zeros(10)\n","false_negative = np.zeros(10)\n","accuracy = 0\n","ctn = 0\n","for X, y in iter(test_loader):\n","    X = X.to(device)\n","    y = y.to(device)\n","    with torch.no_grad():\n","        y_pred = model(X).max(dim=1)[1]\n","    for i in range(10):\n","        for pred, real in zip(y_pred, y):\n","            if real == i:\n","                if pred == real:\n","                    true_positive[i] += 1\n","                else:\n","                    false_negative[i] += 1\n","            else:\n","                if pred == i:\n","                    false_positive[i] += 1\n","                else:\n","                    true_negative[i] += 1\n","            \n","    accuracy += torch.sum(y_pred == y).item()\n","    ctn += len(y)\n","print(\"Overall accuracy\", accuracy / ctn)\n","print(\"Precision\", true_positive / (true_positive + false_positive))\n","print(\"Recall\", true_positive / (true_positive + false_negative))\n","print(\"Mean Precision\", np.mean(true_positive / (true_positive + false_positive)))\n","print(\"Mean Recall\", np.mean(true_positive / (true_positive + false_negative)))"],"execution_count":null,"outputs":[]},{"source":["type | l2 reg | epochs |           acc |\n","-----|--------|--------|---------------|\n","my old |    0 |     12 |        0.7355 |\n","vgg |    0 |     15 |        0.7607 |\n","vgg | 1e-5 |     15 |        0.7610 |\n","vgg | 1e-4 |     15 |        0.7673 |\n","vgg | 1e-3 |     20 |        0.7673 |\n","vgg | 1e-2 |     20 |        0.7673 |\n","vgg + batchnorm @ conv + drop @ fc | 0 | 10 | 0.7975 |\n","vgg + batchnorm @ conv + drop @ fc | 1e-3 | 15 | 0.8152 |\n","vgg + drop & BN @ conv + drop @ fc | 1e-3 | 15 | 0.8228 |\n","resnet + batchnorm @ conv | 1e-4 | 10 | 0.8029 |"],"cell_type":"markdown","metadata":{}}]}