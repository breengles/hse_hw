{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tG3PGp3KLZoD"
   },
   "source": [
    "# This cat does not exist\n",
    "__Суммарное количество баллов: 10__\n",
    "\n",
    "__Решение отправлять на `ml.course.practice@gmail.com`__\n",
    "\n",
    "__Тема письма: `[ML][MS][HW06] <ФИО>`, где вместо `<ФИО>` указаны фамилия и имя__\n",
    "\n",
    "Цель этого задания - создать котов, которых не существует. В ходе данного задания вы обучите DCGAN и VAE, которые являются одними из первых генеративных моделей. Для этого задания вам наверняка потребуется GPU с CUDA, поэтому рекомендуется использовать Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dp0aOFOrLZoJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ia5Mh4DpLZoK"
   },
   "outputs": [],
   "source": [
    "def random_noise(batch_size, channels, side_size):\n",
    "    return torch.randn(batch_size, channels, side_size, side_size).cuda()\n",
    "\n",
    "def imagewide_average(x):\n",
    "    return x.mean(dim=(-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzjaQBlSLZoK"
   },
   "outputs": [],
   "source": [
    "def visualise(imgs, rows=2, pic_save_path=None):\n",
    "    imgs = (imgs.transpose(1, 3) + 1) / 2\n",
    "    imgs = torch.cat([imgs[i::rows] for i in range(rows)], dim=1)\n",
    "    cols = len(imgs)\n",
    "    imgs = (torch.cat(list(imgs), dim=1)).cpu().numpy()[:, :, ::-1]\n",
    "    plt.figure(figsize=(cols * 1.5, rows * 1.5))\n",
    "    plt.imshow(imgs)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if pic_save_path is not None:\n",
    "        plt.savefig(pic_save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cY7GFJXgLZoK"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path_to_dataset=\"cat_136\", h_size=64, w_size=64, to_flip=False, **kwargs):\n",
    "        self.photo_names = os.listdir(path_to_dataset)\n",
    "        self.path_base = path_to_dataset\n",
    "        self.h_size = h_size\n",
    "        self.w_size = w_size\n",
    "        self.to_flip = to_flip\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.path_base + \"/\" + self.photo_names[index]\n",
    "        img = cv2.imread(path) # 136 x 136\n",
    "        crop_rate = 8\n",
    "        x_crop = random.randint(0, crop_rate)\n",
    "        y_crop = random.randint(0, crop_rate)\n",
    "        img = img[x_crop:img.shape[0] - crop_rate + x_crop, y_crop:img.shape[1] - crop_rate + y_crop]\n",
    "\n",
    "        if self.to_flip:\n",
    "            p = np.random.uniform()\n",
    "            img = img[:, ::-1] if p >= 0.5 else img\n",
    "\n",
    "        img = cv2.resize(img, (self.h_size, self.w_size), interpolation=cv2.INTER_CUBIC)\n",
    "        return 2 * torch.tensor(img).float().transpose(0, 2) / 255. - 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.photo_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-obhziTLZoL"
   },
   "outputs": [],
   "source": [
    "dataset = MyDataset()\n",
    "visualise(torch.cat([dataset[i].unsqueeze(0) for i in [3, 15, 182, 592, 394, 2941]], dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cwefkXgLZoL"
   },
   "source": [
    "### Задание 1 (2 балла)\n",
    "Для начала реализуем генератор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/DCGAN.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ConvTranspose2D`, `nn.ReLU`, а также функция `F.interpolate`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `start_size`, `latent_channels`, `start_channels` и `upsamplings`. Первые два аргумента отвечают за размер случайного шума, из которого в последствии будет сгенерирована картинка. `start_channels` отвечает за то, сколько каналов должно быть в картинке перед тем, как к ней будут применены upsampling блоки. `upsamplings` - это количество upsampling блоков, которые должны быть применены к картинке. В каждом таком локе количество каналов уменьшается в два раза.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход `batch_size`, генерирует `batch_size` картинок из случайного шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_M61VtILZoM"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, start_size=2, latent_channels=32, start_channels=1024, upsamplings=6, **kwargs):\n",
    "        super().__init__()\n",
    "        self.start_size = start_size\n",
    "        self.latent_channels = latent_channels\n",
    "        self.start_channels = start_channels\n",
    "        self.upsamplings = upsamplings\n",
    "\n",
    "        self.prep = nn.Conv2d(self.latent_channels, self.start_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.up = nn.Sequential(\n",
    "            *[self._up(self.start_channels // 2 ** i) for i in range(self.upsamplings)]\n",
    "        )\n",
    "        self.finale = self._finale(self.start_channels // 2 ** self.upsamplings)\n",
    "    \n",
    "    def _up(self, c_in):\n",
    "        c_out = c_in // 2\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_out, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c_out, affine=False),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "    \n",
    "    def _finale(self, c_in):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(c_in, 3, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, batch_size: int):\n",
    "        out = torch.randn((batch_size, self.latent_channels, self.start_size, self.start_size)).cuda()\n",
    "\n",
    "        out = self.prep(out)\n",
    "        out = self.up(out)\n",
    "        out = self.finale(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFTyqbAULZoM"
   },
   "source": [
    "### Задание 2 (2 балла)\n",
    "Для начала реализуем дискриминатор для нашего DCGAN. Предлагается использовать следующую архитектуру:\n",
    "\n",
    "![](imgs/Disc_DCGAN.png)\n",
    "\n",
    "Для ее реализации вам потребуются модули `nn.BatchNorm2d`, `nn.Conv2d`, `nn.ReLU` и `nn.Sigmoid`.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `start_channels` и `downsamplings`. `start_channels` определяет количество каналов, которые должны быть в изображении перед применением downsampling блоков.\n",
    "\n",
    "\n",
    "* `forward` - принимает на вход `x` - тензор с картинками. Возвращает вектор с размерностью `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpZpwst8LZoM"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, w_size, h_size, downsamplings=6, start_channels=8, **kwargs):\n",
    "        super().__init__()\n",
    "        self.downsamplings = downsamplings\n",
    "        self.start_channels = start_channels\n",
    "        self.prep = nn.Conv2d(3, start_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.down = nn.Sequential(\n",
    "            *[self._down(self.start_channels * 2 ** i) for i in range(self.downsamplings)]\n",
    "        )\n",
    "        self.finale = self._finale(self.start_channels * 2 ** self.downsamplings, w_size, h_size)\n",
    "\n",
    "    def _down(self, c_in):\n",
    "        c_out = c_in * 2\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c_out, affine=False),\n",
    "            # nn.Dropout(p=0.25),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "    \n",
    "    def _finale(self, c_in,w_size, h_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c_in * w_size * h_size // 2 ** (2 * self.downsamplings), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.prep(X)\n",
    "        out = self.down(out)\n",
    "        return self.finale(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lR-qLTcmLZoM"
   },
   "outputs": [],
   "source": [
    "def train_gan(config):\n",
    "    name = config[\"name\"]\n",
    "    if not os.path.exists(f\"models/gan/{name}\"):\n",
    "        os.mkdir(f\"models/gan/{name}\")\n",
    "        os.mkdir(f\"models/gan/{name}/generators\")\n",
    "        os.mkdir(f\"models/gan/{name}/discriminators\")\n",
    "    gen_dir = f\"models/gan/{name}/generators\"\n",
    "    disc_dir = f\"models/gan/{name}/discriminators\"\n",
    "\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "\n",
    "    generator = Generator(**config).cuda()\n",
    "    discriminator = Discriminator(**config).cuda()\n",
    "    visualise_every = 10\n",
    "\n",
    "    lr = config[\"lr\"]\n",
    "    gen_optim = Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    disc_optim = Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    dataset = MyDataset(**config)\n",
    "\n",
    "    t = tqdm(range(epochs))\n",
    "    for ep in t:\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "        total_batches = 0\n",
    "        gen_loss_avg = 0\n",
    "        disc_loss_avg = 0\n",
    "\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if len(batch) < batch_size:\n",
    "                continue\n",
    "            total_batches += 1\n",
    "            # Positive update\n",
    "            batch = batch.cuda()\n",
    "            pred = discriminator(batch)\n",
    "            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n",
    "            disc_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            disc_optim.step()\n",
    "            disc_loss_avg += loss.item()\n",
    "\n",
    "            # Negative update\n",
    "            batch = generator(batch_size).detach()\n",
    "            pred = discriminator(batch)\n",
    "            loss = F.binary_cross_entropy(pred, torch.zeros_like(pred))\n",
    "            disc_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            disc_optim.step()\n",
    "            disc_loss_avg += loss.item()\n",
    "\n",
    "            # Generator update\n",
    "            batch = generator(batch_size)\n",
    "            pred = discriminator(batch)\n",
    "            loss = F.binary_cross_entropy(pred, torch.ones_like(pred))\n",
    "            gen_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            gen_optim.step()\n",
    "            gen_loss_avg += loss.item()\n",
    "        \n",
    "        if (ep + 1) % visualise_every == 0:\n",
    "            print(f\"Epoch {ep + 1} | Discriminator loss: {disc_loss_avg / total_batches} | Generator loss: {gen_loss_avg / total_batches}\")\n",
    "            if config[\"pic_save_path\"]:\n",
    "                pic_path = config[\"pic_save_path\"] + \"/\" + config[\"name\"] + f\"/{ep + 1}.png\"\n",
    "            with torch.no_grad():\n",
    "                visualise(generator(4), rows=2, pic_save_path=pic_path)\n",
    "            torch.save(generator, f\"{gen_dir}/{ep + 1}\")\n",
    "            torch.save(discriminator, f\"{disc_dir}/{ep + 1}\")\n",
    "\n",
    "        t.set_description(f\"Dloss = {disc_loss_avg / total_batches:.4f} | Gloss = {gen_loss_avg / total_batches:.4f}\")\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = {\n",
    "    \"name\": \"cat_128\",\n",
    "    \"pic_save_path\": \"pics/gan\",\n",
    "    \"path_to_dataset\": \"cat_136\",\n",
    "    \"h_size\": 128,\n",
    "    \"w_size\": 128,\n",
    "    \"epochs\": 300,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    \"to_flip\": True,\n",
    "}\n",
    "gen = train_gan(cat)\n",
    "with torch.no_grad():\n",
    "    visualise(gen(100), rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goose = {\n",
    "#     \"name\": \"goose_128\",\n",
    "#     \"pic_save_path\": \"pics/gan\",\n",
    "#     \"path_to_dataset\": \"goose\",\n",
    "#     \"h_size\": 128,\n",
    "#     \"w_size\": 128,\n",
    "#     \"epochs\": 1000,\n",
    "#     \"batch_size\": 8,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"to_flip\": True,\n",
    "# }\n",
    "# gen = train_gan(goose)\n",
    "# with torch.no_grad():\n",
    "#     visualise(gen(100), rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in [10, 50, 100, 150, 200, 250, 260]:\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    gen = torch.load(f\"models/gan/cat_128/generators/{epoch}\")\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        visualise(gen(27), rows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in [10, 100, 200, 400, 600, 700]:\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    gen = torch.load(f\"models/gan/goose_128/generators/{epoch}\")\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        visualise(gen(27), rows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOPUdWezLZoN"
   },
   "source": [
    "### Задание 3 (5 баллов)\n",
    "Теперь посмотрим на другую модель: Variational Autoencoder. В отличии от GAN, в котором генератор пытается себя обмануть дискриминатор, а дискриминатор старается не быть обманутым, VAE решает задачу реконструкции элемента множества X с применением регуляризации в латентном пространстве. \n",
    "\n",
    "Полностью архитектура выглядит так:\n",
    "![](imgs/VAE.png)\n",
    "\n",
    "Из нее можно выделить две части: Encoder (по изображению возвращает mu и sigma) и Decoder (по случайному шуму восстанавливает изображение). На высоком уровне VAE можно представить так:\n",
    "\n",
    "![](imgs/VAE_highlevel.png)\n",
    "\n",
    "В данном задании вам необходимо реализовать полную архитектуру VAE.\n",
    "\n",
    "#### Методы\n",
    "* `__init__` - принимает на вход `img_size`, `downsamplings`, `latent_size`, `linear_hidden_size`, `down_channels` и `up_channels`. `img_size` - размер стороны входного изображения. `downsamplings` - количество downsampling (и upsampling) блоков. `latent_size` - размер латентного пространства, в котором в который будет закодирована картинка. `linear_hidden_size` количество нейронов на скрытом слое полносвязной сети в конце encoder'а. Для полносвязной сети decoder'а это число стоит умножить на 2. `down_channels` - количество каналов, в которое будет преобразовано трехцветное изображение перед применением `downsampling` блоков. `up_channels` - количество каналов, которое должно получиться после применения всех upsampling блоков.\n",
    "\n",
    "* `forward` - принимает на вход `x`. Считает распределение $N(\\mu, \\sigma^2)$ и вектор $z \\sim N(\\mu, \\sigma^2)$. Возвращает $x'$ - восстановленную из вектора $z$ картинку и $D_{KL}(N(\\mu, \\sigma^2), N(0, 1)) = 0.5 \\cdot (\\sigma^2 + \\mu^2 - \\log \\sigma^2 - 1)$.\n",
    "\n",
    "* `encode` - принимает на вход `x`. Возвращает вектор из распределения $N(\\mu, \\sigma^2)$.\n",
    "\n",
    "* `decode` - принимает на вход `z`. Возвращает восстановленную по вектору картинку.\n",
    "\n",
    "\n",
    "#### Если хочется улучшить качество\n",
    "https://arxiv.org/pdf/1906.00446.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfVEsDs7LZoN"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, img_size=128, downsamplings=5, latent_size=512, down_channels=8, up_channels=16):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, down_channels, kernel_size=1, stride=1, padding=0),\n",
    "            *[self._down(down_channels * 2 ** i) for i in range(downsamplings)],\n",
    "            nn.Conv2d(down_channels * 2 ** downsamplings, latent_size * 2, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        dec_chn = up_channels * 2 ** downsamplings\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(latent_size, dec_chn, kernel_size=1, stride=1, padding=0),\n",
    "            *[self._up(dec_chn // 2 ** i) for i in range(downsamplings)],\n",
    "            nn.Conv2d(up_channels, 3, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def _down(self, c_in):\n",
    "        c_out = c_in * 2\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            # nn.Dropout(),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def _up(self, c_in):\n",
    "        c_out = c_in // 2\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_out, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, sigma = torch.split(self.encoder(x), self.latent_size, dim=1)\n",
    "        sigma = torch.exp(sigma)\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        kld = 0.5 * (sigma ** 2 + mu ** 2 - 2 * torch.log(sigma) - 1)\n",
    "        return self.decoder(z), kld\n",
    "    \n",
    "    def encode(self, x):\n",
    "        mu, sigma = torch.split(self.encoder(x), self.latent_size, dim=1)\n",
    "        z = mu + torch.exp(sigma) * torch.randn_like(sigma)\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_uy9QF0LZoO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_vae(config):\n",
    "    name = config[\"name\"]\n",
    "    if not os.path.exists(f\"models/vae/{name}\"):\n",
    "        os.mkdir(f\"models/vae/{name}\")\n",
    "    model_dir = f\"models/vae/{name}\"\n",
    "\n",
    "    vae = VAE(downsamplings=7)\n",
    "    vae.cuda()\n",
    "\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    vae_optim = Adam(vae.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    dataset = MyDataset(**config)\n",
    "\n",
    "    test_imgs_1 = torch.cat([dataset[i].unsqueeze(0) for i in (0, 34, 76, 1509)])\n",
    "    test_imgs_2 = torch.cat([dataset[i].unsqueeze(0) for i in (734, 123, 512, 3634)])\n",
    "\n",
    "    t = tqdm(range(epochs))\n",
    "\n",
    "    for ep in t:\n",
    "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "        total_batches = 0\n",
    "        rec_loss_avg = 0\n",
    "        kld_loss_avg = 0\n",
    "\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if len(batch) < batch_size:\n",
    "                continue\n",
    "            total_batches += 1\n",
    "            x = batch.cuda()\n",
    "            x_rec, kld = vae(x)\n",
    "            img_elems = float(np.prod(list(batch.size())))\n",
    "            kld_loss = kld.sum() / batch_size\n",
    "            rec_loss = ((x_rec - x)**2).sum() / batch_size\n",
    "            loss = rec_loss + 0.1 * kld_loss # https://openreview.net/forum?id=Sy2fzU9gl\n",
    "            vae_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            vae_optim.step()\n",
    "            kld_loss_avg += kld_loss.item()\n",
    "            rec_loss_avg += rec_loss.item()\n",
    "        \n",
    "        if (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1} | Reconstruction loss: {rec_loss_avg / total_batches} | KLD loss: {kld_loss_avg / total_batches}\")\n",
    "            if config[\"pic_save_path\"]:\n",
    "                pic_path = config[\"pic_save_path\"] + \"/\" + config[\"name\"] + f\"/{ep + 1}.png\"\n",
    "            torch.save(vae, f\"{model_dir}/{ep + 1}\")\n",
    "            with torch.no_grad():\n",
    "                z_1 = vae.encode(test_imgs_1.cuda())\n",
    "                z_2 = vae.encode(test_imgs_2.cuda())\n",
    "                x_int = []\n",
    "                for i in range(9):\n",
    "                    z = (i * z_1 + (8 - i) * z_2) / 8\n",
    "                    x_int.append(vae.decode(z))\n",
    "                x_int = torch.cat(x_int)\n",
    "                visualise(x_int, rows=len(test_imgs_1), pic_save_path=pic_path)\n",
    "                z_rand = torch.randn_like(z_1)\n",
    "                x_int = vae.decode(z_rand)\n",
    "                visualise(x_int, rows=len(test_imgs_1)//2, pic_save_path=pic_path)\n",
    "        t.set_description(f\"Rloss = {rec_loss_avg / total_batches:.4f} | KLDloss = {kld_loss_avg / total_batches:.4f}\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat = {\n",
    "#     \"name\": \"cat_128\",\n",
    "#     \"path_to_dataset\": \"cat_136\",\n",
    "#     \"pic_save_path\": \"pics/vae\",\n",
    "#     \"h_size\": 128,\n",
    "#     \"w_size\": 128,\n",
    "#     \"epochs\": 1000,\n",
    "#     \"batch_size\": 8,\n",
    "#     \"lr\": 1e-4,\n",
    "#     \"to_flip\": True,\n",
    "# }\n",
    "# vae_cat = train_vae(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = {\n",
    "    \"name\": \"cat_128\",\n",
    "    \"path_to_dataset\": \"cat_136\",\n",
    "    \"pic_save_path\": \"pics/vae\",\n",
    "    \"h_size\": 128,\n",
    "    \"w_size\": 128,\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 1e-4,\n",
    "    \"to_flip\": True,\n",
    "}\n",
    "dataset = MyDataset(**cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in [10, 200, 400, 600, 800, 1000]:\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    vae = torch.load(f\"models/vae/cat_128/{epoch}\")\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        x_int = vae.decode(torch.randn((27, 512, 1, 1)).cuda())\n",
    "        visualise(x_int, rows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = torch.load(f\"models/vae/cat_128/1000\")\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    x_int = vae.decode(torch.randn((100, 512, 1, 1)).cuda())\n",
    "    visualise(x_int, rows=10)"
   ]
  },
  {
   "source": [
    "Странно, то тут котов сильнее размазывает, т.е. при загрузки модельки, чем при генерации в ноутбуке сразу после обучения в этом же ноутбуке\n",
    "\n",
    "Вот для примера, как оно было для того же кол-ва эпох в конце:\n",
    "\n",
    "![](pics/vae_old_1000.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw06_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('hse_ml')",
   "metadata": {
    "interpreter": {
     "hash": "52f65c1dba17a6893288d8494a539e55e57ab164b424d6c977bf5446fbca9423"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}