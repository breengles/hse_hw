{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd071f4a8a05e914619e6ce849cc212b7860cf82baee4b36c2033f2d6275d70daac",
   "display_name": "Python 3.8.5 64-bit ('ml': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "PRINT_EVERY = NUM_EPOCHS // 100 if NUM_EPOCHS > 100 else 1\n",
    "TEACHER_PATH = \"./teacher.pth\"\n",
    "LR = 0.01\n",
    "NUM_WORKERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(net, loader):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    net.train()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, \n",
    "                                          batch_size=BATCH_SIZE, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=NUM_WORKERS)\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=BATCH_SIZE, \n",
    "                                         shuffle=False, \n",
    "                                         num_workers=NUM_WORKERS)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = models.vgg16_bn(pretrained=True)\n",
    "\n",
    "# for param in teacher.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "teacher.classifier[6] = nn.Linear(4096,10)\n",
    "\n",
    "# teacher.classifier[6] = nn.Linear(4096,1024)\n",
    "# teacher.classifier.add_module(\"head\", nn.Linear(1024, 10))\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(teacher.parameters(), lr=LR, momentum=0.9)\n",
    "# optimizer = optim.Adam(teacher.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/100 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4eaebddc6ea41cb92029114d2bdadef"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1] loss: 7.151553208 | accuracy: 28.00%\n",
      "[2] loss: 2.321216430 | accuracy: 29.84%\n"
     ]
    }
   ],
   "source": [
    "if not path.exists(TEACHER_PATH):\n",
    "    t = tqdm(range(NUM_EPOCHS))\n",
    "    teacher.to(DEVICE)\n",
    "    for epoch in t:\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = teacher(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % PRINT_EVERY == 0:\n",
    "            acc = get_acc(teacher, testloader)\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.9f} | accuracy: {acc:0.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "    torch.save(teacher.state_dict(), TEACHER_PATH)\n",
    "else:\n",
    "    print(\"Loaded saved teacher model\")\n",
    "    teacher.load_state_dict(torch.load(TEACHER_PATH))\n",
    "    teacher.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_NUM_PARAMS = sum(p.numel() for p in teacher.parameters())\n",
    "print(TEACHER_NUM_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_ACC = get_acc(teacher, testloader)\n",
    "print(f\"Accuracy: {TEACHER_ACC} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data    \n",
    "        outputs = teacher(images.to(DEVICE))    \n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        for label, prediction in zip(labels, predictions.cpu()):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "  \n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f\"Accuracy for class {classname} is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dVGG(nn.Module):\n",
    "    def __init__(self, a=0, kind=1):\n",
    "        super().__init__()\n",
    "        self.one = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(16 * 5 * 5, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        self.two = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(16 * 10 * 10, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        self.three = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.BatchNorm2d(512),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.criterion_mse = torch.nn.MSELoss()\n",
    "        self.criterion_ce = torch.nn.CrossEntropyLoss()\n",
    "        self.a = a\n",
    "        self.kind = kind\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.kind == 1:\n",
    "            out = self.one(x)\n",
    "        elif self.kind == 2:\n",
    "            out = self.two(x)\n",
    "        elif self.kind == 3:\n",
    "            out = self.three(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected `kind`\")\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def loss(self, output, teacher_prob, real_label):\n",
    "        return self.a * self.criterion_ce(output, real_label) + (1 - self.a) * self.criterion_mse(output, teacher_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(kind=3, opt=\"sgd\", coef=1):\n",
    "    print(f\"=== {kind} | {opt} ===\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net = dVGG(1, kind).to(DEVICE)\n",
    "    \n",
    "    if opt.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "    if opt.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in tqdm(range(int(NUM_EPOCHS * coef))):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        if (epoch + 1) % PRINT_EVERY == 0:\n",
    "            acc = get_acc(net, testloader)\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.5f} | accuracy: {acc:0.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    baseline_acc = get_acc(net, testloader)\n",
    "    torch.save(net.state_dict(), f\"./baseline_{a}_{kind}_{opt}.pth\")\n",
    "\n",
    "    print(f\"=== Finished: {kind} | {opt} ===\")\n",
    "    print(\"\\Baseline accuracy on test:\", baseline_acc, \"%\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distil(a=0, kind=3, opt=\"sgd\", coef=1):\n",
    "    print(f\"=== {a} | {kind} | {opt} ===\")\n",
    "    net = dVGG(a, kind).to(DEVICE)\n",
    "    if opt.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\n",
    "    if opt.lower() == \"adam\":\n",
    "        optimizer = optim.Adam(net.parameters(), lr=LR)\n",
    "\n",
    "    for epoch in tqdm(range(int(NUM_EPOCHS * coef))):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs_teacher = teacher(inputs)\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss = net.loss(outputs, outputs_teacher, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        if (epoch + 1) % PRINT_EVERY == 0:\n",
    "            acc = get_acc(net, testloader)\n",
    "            print(f'[{epoch + 1}] loss: {running_loss / len(trainloader):0.5f} | accuracy: {acc:0.2f}%')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    learner_acc = get_acc(net, testloader)\n",
    "    torch.save(net.state_dict(), f\"./distilled_{a}_{kind}_{opt}.pth\")\n",
    "\n",
    "    learner_num_params = sum(p.numel() for p in net.parameters())\n",
    "    print(f\"=== Finished: {a} | {kind} | {opt} ===\")\n",
    "    print(\"\\tTotal number of teacher params:\", TEACHER_NUM_PARAMS)\n",
    "    print(\"\\tTotal number of learner params:\", learner_num_params)\n",
    "    print(\"\\tTotal reduction:\", (TEACHER_NUM_PARAMS - learner_num_params) / TEACHER_NUM_PARAMS, \"%\")\n",
    "    print(\"\\tTeacher accuracy on test:\", TEACHER_ACC, \"%\")\n",
    "    print(\"\\tLearner accuracy on test:\", learner_acc, \"%\")\n",
    "    print(\"\\tDiff:\", TEACHER_ACC - learner_acc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_baseline(coef=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in (0, 0.1, 0.5, 0.7, 0.9):\n",
    "    for opt in (\"sgd\", \"adam\"):\n",
    "        distil(a=a, kind=3, opt=opt, coef=1)"
   ]
  }
 ]
}